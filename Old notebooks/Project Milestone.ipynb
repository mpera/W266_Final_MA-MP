{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W266 Final Project: Project Milestone\n",
    "\n",
    "### Classifying the Political Ideology of News Articles\n",
    "\n",
    "#### Matt Acconciamessa and Megan Pera\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, Cleaning and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and save data into liberal, conservative and neutral objects\n",
    "[lib, con, neutral] = pickle.load(open('ibcData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data samples, by classification\n",
    "print ('Liberal examples (out of ', len(lib), ' sentences): ')\n",
    "for tree in lib[0:5]:\n",
    "    print(tree.get_words())\n",
    "    \n",
    "print ('\\nConservative examples (out of ', len(con), ' sentences): ')\n",
    "for tree in con[0:5]:\n",
    "    print (tree.get_words())\n",
    "    \n",
    "print ('\\nNeutral examples (out of ', len(neutral), ' sentences): ')\n",
    "for tree in neutral[0:5]:\n",
    "    print (tree.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Formatting data into workable arrays\n",
    "liberal = np.array(lib)\n",
    "conserv = np.array(con)\n",
    "neut = np.array(neutral)\n",
    "\n",
    "# Seprating data and labels\n",
    "def separate_data_and_labels(label_class):\n",
    "    labels = []\n",
    "    data = []\n",
    "    for i in range(len(label_class)):\n",
    "        for node in label_class[i]:\n",
    "            if hasattr(node, 'label'):\n",
    "                data.append(node.get_words())\n",
    "                labels.append(node.label)\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "lib_data, lib_labs = separate_data_and_labels(liberal)\n",
    "con_data, con_labs = separate_data_and_labels(conserv)\n",
    "neut_data, neut_labs = separate_data_and_labels(neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Examples:')\n",
    "print ('\\n Liberal')\n",
    "print(lib_data[0],'\\n',lib_labs[0:10])\n",
    "print ('\\n Conservative')\n",
    "print(con_data[0],'\\n',con_labs[0:10])\n",
    "print ('\\n Neutral')\n",
    "print(neut_data[0],'\\n',neut_labs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combining into one dataset\n",
    "data_all = np.concatenate((neut_data, lib_data, con_data), axis=0)\n",
    "labs_all = np.concatenate((neut_labs, lib_labs, con_labs), axis=0)\n",
    "\n",
    "print (data_all.shape)\n",
    "print (labs_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly mixing data&labels so that they can be split into test and train\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "data_all, labs_all = shuffle_in_unison(data_all, labs_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Matt\n",
    "# Should we be including a dev set as well, and reserve a test set for the very last iteration of each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into test (20%) and train (80%)\n",
    "slice = int(.8*labs_all.shape[0])\n",
    "data_train = data_all[:slice]\n",
    "labs_train = labs_all[:slice]\n",
    "data_test = data_all[slice:]\n",
    "labs_test = labs_all[slice:]\n",
    "print(labs_all.shape)\n",
    "print(labs_test.shape)\n",
    "print(labs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turning dataset into word tokens\n",
    "count_vect = CountVectorizer()\n",
    "data = count_vect.fit_transform(data_train).toarray()\n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# Counting the number of times each word appears\n",
    "np.clip(data,0,1, out = data) #make sure each word only appears once in the array\n",
    "dist = np.sum(data, axis = 0) #sum the columns\n",
    "counts = list(zip(vocab,dist)) #zip counts and words together\n",
    "\n",
    "# Total vocab size and word count\n",
    "print(\"Total word count:\",np.sum(dist))\n",
    "print(\"Vocabulary size:\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Printing out the 20 most popular words\n",
    "counts = sorted(counts, key=lambda x: x[1], reverse=True) \n",
    "counts[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting top 50 results\n",
    "ordered = list(zip(*counts))\n",
    "x = ordered[0][:50] #counts\n",
    "y = ordered[1][:50] #words\n",
    "\n",
    "# Plotting figure\n",
    "fig = plt.figure(figsize=(15.0,6.0))\n",
    "indexes = np.arange(50)\n",
    "width = .25\n",
    "plt.bar(indexes, y, width)\n",
    "plt.xticks(indexes + width * 0.5, x,rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model: Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This model predicts the political leanings of sentences and sub-sentences\n",
    "\n",
    "# Training the model\n",
    "vect = CountVectorizer()\n",
    "train_vocab = vect.fit_transform(data_train)\n",
    "test_vocab = vect.transform(data_test)\n",
    "\n",
    "# Scoring the model\n",
    "print(\"\")\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "for a in [0.0001, 0.01, .05, 0.1, 0.2, 1.0]:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train_vocab, labs_train)\n",
    "    mnbpreds = mnb.predict(test_vocab)\n",
    "    print(\"alpha:\", a, \"F1:\", metrics.f1_score(labs_test,mnbpreds,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Showing examples for alpha = 0.001\n",
    "mnb = MultinomialNB(alpha=0.001)\n",
    "mnb.fit(train_vocab, labs_train)\n",
    "mnbpreds = mnb.predict(test_vocab)\n",
    "mnbpred_prob = mnb.predict_proba(test_vocab)\n",
    "probs = list(zip(data_test.tolist(),mnbpreds.tolist(),mnbpred_prob.tolist()))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print('Sentence:',probs[i][0])\n",
    "    print('Actual Label:',labs_test[i])\n",
    "    print('Predicted Label:',probs[i][1])\n",
    "    print('Predicted Label Probability:', max(probs[i][2]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding and printing out mistakes\n",
    "errors = []\n",
    "for i in range(0,len(probs)):\n",
    "    if labs_test[i] == probs[i][1]:\n",
    "        pass\n",
    "    else:\n",
    "        errors.append(i)\n",
    "        \n",
    "print('MNB missclassified',len(errors),'sentences','\\n')\n",
    "\n",
    "for i in errors[0:5]:\n",
    "    print('Sentence:',probs[i][0])\n",
    "    print('Actual Label:',labs_test[i])\n",
    "    print('Predicted Label:',probs[i][1])\n",
    "    print('Predicted Label Probability:', max(probs[i][2]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building a pipeline for News Articles to Score with Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'newspaper3k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-155e6128538f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnewspaper3k\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'newspaper3k'"
     ]
    }
   ],
   "source": [
    "from newspaper3k import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
