{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W266 Final Project: Project Milestone\n",
    "\n",
    "### Classifying the Political Ideology of News Articles\n",
    "\n",
    "#### Matt Acconciamessa and Megan Pera\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megan/anaconda/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/megan/anaconda/lib/python3.4/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, Cleaning and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and save data into liberal, conservative and neutral objects\n",
    "[lib, con, neutral] = pickle.load(open('ibcData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal examples (out of  2025  sentences): \n",
      "Forcing middle-class workers to bear a greater share of the cost of government weakens their support for needed investments and stirs resentment toward those who depend on public services the most .\n",
      "Because it would not be worthwhile to bring a case for $ 30.22 , the arbitration clause would , as a practical matter , deny the Concepcions any relief and , more important , eliminate a class action that might punish AT&T for its pattern of fraudulent behavior .\n",
      "Indeed , Lind argues that high profits and high wages reinforce each other because workers then have the wherewithal to buy the products they are making .\n",
      "In fairness , it should be noted that he devotes an entire chapter to New York Times political columnist Maureen Dowd , a liberal who makes much of the outsized rivalries , jealousies , and personalities that dominate American politics .\n",
      "Psychological tactics are social control techniques that operate at the level of the mind , with the goal of creating fear and making it difficult for protesters to successfully mobilize .\n",
      "\n",
      "Conservative examples (out of  1701  sentences): \n",
      "Gore is getting rich from environmentalism , not just by being paid a whopping $ 175,000 per speech but by using political pressure to force government policy in a direction that benefits his business interests .\n",
      "The Federal Housing Finance Regulatory Reform Act of 2008 should have been an easy sell , since it purportedly aimed to assist homeowners , a more popular ( or at least more sentimentalized ) subset of Americans than greedy Wall Street tycoons .\n",
      "Yet for all its submerged class snobbery and anti-intellectualism disguised as cool detachment , the ultimate failure of the Washington media lies less with the personal failings of its elite members than its structural inadequacy .\n",
      "Rumsfeld then went on to discuss how China 's lack of transparency with respect to its defense expenditures and activities raises doubts in the region about China 's intentions .\n",
      "You never hear from the co-conspirators of the left-wing media how many innocent victims are dead , raped , and mutilated as a direct result of these left-wing policies and insane anti-gun laws .\n",
      "\n",
      "Neutral examples (out of  600  sentences): \n",
      "In this country , the beneficiaries of Apple 's success are , first , the designers , who have done wonders working with Steve Jobs to produce products that are beautiful and effective .\n",
      "The problem with this argument is that China reports about 68 percent of the world 's aquaculture production , and the FAO , which has been burned by inflated Chinese statistics before , expresses doubt about its stated production and growth rates . ''\n",
      "The soil exhaustion caused by the plantation system , as well as the relatively low productivity of forced labor , compelled planters to seek new lands to exploit .\n",
      "The same complexity that leads to such malfunctions also creates vulnerabilities that human agents can use to make computer systems operate in unintended ways .\n",
      "Threads of new awkwardness stretch out between them , and nature itself winks behind their backs and plays nasty tricks on them , scattering yellow clods of asters and groundsel , blanketing purple clover and pink flax , erecting stalks of huge -- but smelly -- purple arum flowers , sprinkling red buttercups , and hanging baby oranges and lemons on the trees around them .\n"
     ]
    }
   ],
   "source": [
    "# Data samples, by classification\n",
    "print ('Liberal examples (out of ', len(lib), ' sentences): ')\n",
    "for tree in lib[0:5]:\n",
    "    print(tree.get_words())\n",
    "    \n",
    "print ('\\nConservative examples (out of ', len(con), ' sentences): ')\n",
    "for tree in con[0:5]:\n",
    "    print (tree.get_words())\n",
    "    \n",
    "print ('\\nNeutral examples (out of ', len(neutral), ' sentences): ')\n",
    "for tree in neutral[0:5]:\n",
    "    print (tree.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Formatting data into workable arrays\n",
    "liberal = np.array(lib)\n",
    "conserv = np.array(con)\n",
    "neut = np.array(neutral)\n",
    "\n",
    "# Seprating data and labels\n",
    "def separate_data_and_labels(label_class):\n",
    "    labels = []\n",
    "    data = []\n",
    "    for i in range(len(label_class)):\n",
    "        for node in label_class[i]:\n",
    "            if hasattr(node, 'label'):\n",
    "                data.append(node.get_words())\n",
    "                labels.append(node.label)\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "lib_data, lib_labs = separate_data_and_labels(liberal)\n",
    "con_data, con_labs = separate_data_and_labels(conserv)\n",
    "neut_data, neut_labs = separate_data_and_labels(neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\n",
      " Liberal\n",
      "Forcing middle-class workers to bear a greater share of the cost of government weakens their support for needed investments and stirs resentment toward those who depend on public services the most . \n",
      " ['Liberal' 'Liberal' 'Liberal' 'Liberal' 'Liberal' 'Liberal' 'Liberal'\n",
      " 'Liberal' 'Liberal' 'Liberal']\n",
      "\n",
      " Conservative\n",
      "Gore is getting rich from environmentalism , not just by being paid a whopping $ 175,000 per speech but by using political pressure to force government policy in a direction that benefits his business interests . \n",
      " ['Conservative' 'Conservative' 'Conservative' 'Conservative' 'Neutral'\n",
      " 'Neutral' 'Neutral' 'Conservative' 'Liberal' 'Liberal']\n",
      "\n",
      " Neutral\n",
      "In this country , the beneficiaries of Apple 's success are , first , the designers , who have done wonders working with Steve Jobs to produce products that are beautiful and effective . \n",
      " ['Neutral' 'Neutral' 'Neutral' 'Neutral' 'Neutral' 'Neutral' 'Neutral'\n",
      " 'Neutral' 'Neutral' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "print('Examples:')\n",
    "print ('\\n Liberal')\n",
    "print(lib_data[0],'\\n',lib_labs[0:10])\n",
    "print ('\\n Conservative')\n",
    "print(con_data[0],'\\n',con_labs[0:10])\n",
    "print ('\\n Neutral')\n",
    "print(neut_data[0],'\\n',neut_labs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621,)\n",
      "(22621,)\n"
     ]
    }
   ],
   "source": [
    "# Combining into one dataset\n",
    "data_all = np.concatenate((neut_data, lib_data, con_data), axis=0)\n",
    "labs_all = np.concatenate((neut_labs, lib_labs, con_labs), axis=0)\n",
    "\n",
    "print (data_all.shape)\n",
    "print (labs_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly mixing data&labels so that they can be split into test and train\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "data_all, labs_all = shuffle_in_unison(data_all, labs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into test (20%) and train (80%)\n",
    "slice = int(.8*labs_all.shape[0])\n",
    "data_train = data_all[:slice]\n",
    "labs_train = labs_all[:slice]\n",
    "data_test = data_all[slice:]\n",
    "labs_test = labs_all[slice:]\n",
    "print(labs_all.shape)\n",
    "print(labs_test.shape)\n",
    "print(labs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turning dataset into word tokens\n",
    "count_vect = CountVectorizer()\n",
    "data = count_vect.fit_transform(data_train).toarray()\n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# Counting the number of times each word appears\n",
    "np.clip(data,0,1, out = data) #make sure each word only appears once in the array\n",
    "dist = np.sum(data, axis = 0) #sum the columns\n",
    "counts = list(zip(vocab,dist)) #zip counts and words together\n",
    "\n",
    "# Total vocab size and word count\n",
    "print(\"Total word count:\",np.sum(dist))\n",
    "print(\"Vocabulary size:\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Printing out the 20 most popular words\n",
    "counts = sorted(counts, key=lambda x: x[1], reverse=True) \n",
    "counts[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting top 50 results\n",
    "ordered = list(zip(*counts))\n",
    "x = ordered[0][:50] #counts\n",
    "y = ordered[1][:50] #words\n",
    "\n",
    "# Plotting figure\n",
    "fig = plt.figure(figsize=(15.0,6.0))\n",
    "indexes = np.arange(50)\n",
    "width = .5\n",
    "plt.bar(indexes, y, width)\n",
    "plt.xticks(indexes + width * 0.5, x,rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model: Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This model predicts the political leanings of sentences and sub-sentences\n",
    "\n",
    "# Training the model\n",
    "vect = CountVectorizer()\n",
    "train_vocab = vect.fit_transform(data_train)\n",
    "test_vocab = vect.transform(data_test)\n",
    "\n",
    "# Scoring the model\n",
    "print(\"\")\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "for a in [0.0001, 0.01, .05, 0.1, 0.2, 1.0]:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train_vocab, labs_train)\n",
    "    mnbpreds = mnb.predict(test_vocab)\n",
    "    print(\"alpha:\", a, \"F1:\", metrics.f1_score(labs_test,mnbpreds,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Showing examples for alpha = 0.001\n",
    "mnb = MultinomialNB(alpha=0.001)\n",
    "mnb.fit(train_vocab, labs_train)\n",
    "mnbpreds = mnb.predict(test_vocab)\n",
    "mnbpred_prob = mnb.predict_proba(test_vocab)\n",
    "probs = list(zip(data_test.tolist(),mnbpreds.tolist(),mnbpred_prob.tolist()))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print('Sentence:',probs[i][0])\n",
    "    print('Actual Label:',labs_test[i])\n",
    "    print('Predicted Label:',probs[i][1])\n",
    "    print('Predicted Label Probability:', max(probs[i][2]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding and printing out mistakes\n",
    "errors = []\n",
    "for i in range(0,len(probs)):\n",
    "    if labs_test[i] == probs[i][1]:\n",
    "        pass\n",
    "    else:\n",
    "        errors.append(i)\n",
    "        \n",
    "print('MNB missclassified',len(errors),'sentences','\\n')\n",
    "\n",
    "for i in errors[0:5]:\n",
    "    print('Sentence:',probs[i][0])\n",
    "    print('Actual Label:',labs_test[i])\n",
    "    print('Predicted Label:',probs[i][1])\n",
    "    print('Predicted Label Probability:', max(probs[i][2]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring News Articles with Baseline Niave Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Office hour questions\n",
    "\n",
    "We're training a language model on the Brown corpus first, which has its vocabulary. Then, we want to train an LSTM to classify text into liberal, conservative, and neutral buckets using initialized parameters from the Brown-trained language model. \n",
    "1. The Brown vocabulary is likely slightly different than the vocabulary of our ideological training data, which is different from news articles that we are going to score later on. Is this a problem? If so, how do we deal with this to build a unified vocabulary?\n",
    "2. What variables do we need to carry over from the Brown language model? Just final h? What about word embeddings (W in)?\n",
    "3. How to replace the softmax layer from whole vocabulary to three classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"0.12\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import vocabulary, utils, utils_ideo\n",
    "\n",
    "# LSTM code\n",
    "import rnnlm\n",
    "import rnnlm_ideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (924077 tokens)\n",
      "Test set: 11468 sentences (237115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Loading Brown data to train language model to initialize LSTM parameters\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=10000, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Smaller Brown test set to make sure model works\n",
    "mini_train_ids = train_ids[0:1000]\n",
    "mini_test_ids = test_ids[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22621 sentences (2.83457e+06 tokens)\n",
      "Training set: 18096 sentences (2261010 tokens)\n",
      "Test set: 4525 sentences (573561 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Loading ideological data\n",
    "vocab_ideo, train_ids_ideo, test_ids_ideo, train_labs_ideo, test_labs_ideo = utils_ideo.process_data(\n",
    "    data_all, labs_all, split=0.8, V=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Smaller ideo test set to make sure model works\n",
    "mini_train_ids_ideo = train_ids_ideo[0:100]\n",
    "mini_test_ids_ideo = test_ids_ideo[0:100]\n",
    "mini_train_labs_ideo = train_labs_ideo[0:100]\n",
    "mini_test_labs_ideo = test_labs_ideo[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        # Reshape targets to be one long vector\n",
    "        y = y.reshape([-1,1])\n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                    lm.learning_rate_ : learning_rate,\n",
    "                    lm.initial_h_ : h,\n",
    "                    lm.target_y_ : y}\n",
    "        cost, step, h = session.run([loss, train_op, lm.final_h_], feed_dict = feed_dict)\n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "            \n",
    "    final_cost = total_cost / total_batches\n",
    "    \n",
    "    return final_cost, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch_classification(lm, session, batch_iterator, final_h,\n",
    "              train=False, verbose=False,tick_s=10, \n",
    "              learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    #total_logits = np.empty([0,1,10000])\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        \n",
    "        if i == 0:\n",
    "            h = final_h # final state passed from language model\n",
    "\n",
    "        # Reshape targets to be one long vector\n",
    "        y = y.reshape([-1,1])\n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                    lm.learning_rate_ : learning_rate,\n",
    "                    lm.initial_h_ : h,\n",
    "                    lm.target_y_ : y}\n",
    "        cost, step, h = session.run([loss, train_op, lm.final_h_], feed_dict = feed_dict)\n",
    "        \n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "        #total_logits = np.append(total_logits,logits, axis = 0)\n",
    "        \n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    final_cost = total_cost / total_batches\n",
    "            \n",
    "    return final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost, final_state = run_epoch(lm, session, bi, learning_rate=1.0, train=False, verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))\n",
    "\n",
    "# specify classification labels\n",
    "def score_dataset_ideo(cm, session, ids, labels, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator_ideology(ids, labels, batch_size=50, max_time=100)\n",
    "    cost = run_epoch_classification(cm, session, bi, final_state, learning_rate=1.0, train=False, verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up language model and parameters\n",
    "\n",
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "\n",
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, H=200, softmax_ns=200, num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"language_model\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"language_model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:00\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:00:00\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:00:00\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:00:00\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:00:00\n",
      "Train set: avg. loss: 7.539  (perplexity: 1880.33)\n",
      "[epoch 5] None\n",
      "Test set: avg. loss: 7.798  (perplexity: 2436.20)\n",
      "[epoch 5] None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# GENERAL LANGUAGE MODEL for initializing parameters\n",
    "####\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.batch_generator(mini_train_ids, batch_size, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.        \n",
    "        cost, final_state = run_epoch(lm, session, bi, learning_rate=learning_rate, \n",
    "                         train=True, verbose=False, tick_s=3600)\n",
    "        \n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset(lm, session, mini_train_ids, name=\"Train set\"))\n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset(lm, session, mini_test_ids, name=\"Test set\"))\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:00\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:00:00\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:00:00\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:00:00\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:00:00\n",
      "Train set: avg. loss: 2.000  (perplexity: 7.39)\n",
      "[epoch 5] None\n",
      "Test set: avg. loss: 2.733  (perplexity: 15.38)\n",
      "[epoch 5] None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# CLASSIFICATION MODEL for predicting liberal, neutral, conservative\n",
    "####\n",
    "\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    # Restore variables from language model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "\n",
    "        bi = utils.batch_generator_ideology(mini_train_ids_ideo,mini_train_labs_ideo, batch_size, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.        \n",
    "        cost = run_epoch_classification(lm, session, bi, final_state, learning_rate=learning_rate,\n",
    "                         train=True, verbose=False, tick_s=3600)\n",
    "        \n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset_ideo(lm, session, mini_train_ids_ideo,mini_train_labs_ideo, name=\"Train set\"))\n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset_ideo(lm, session, mini_test_ids_ideo,mini_test_labs_ideo, name=\"Test set\"))\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL MODEL END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up classification model and parameters\n",
    "\n",
    "import rnnlm_ideo\n",
    "\n",
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(C = 3, V=10000, H=200, softmax_ns=200, num_layers=2)\n",
    "\n",
    "cm = rnnlm_ideo.RNNLM(**model_params)\n",
    "cm.BuildCoreGraph()\n",
    "cm.BuildTrainGraph()\n",
    "cm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, cm.graph)\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"classification_model\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"classification_model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####\n",
    "# CLASSIFICATION MODEL for predicting liberal, neutral, conservative\n",
    "####\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with cm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "\n",
    "with tf.Session(graph=cm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer) #IS THIS RIGHT??\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        # ADD LABELS HERE\n",
    "        bi = utils.batch_generator_ideology(mini_train_ids_ideo,mini_train_labs_ideo, batch_size, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.        \n",
    "        cost, final_logits = run_epoch_classification(cm, session, bi, state, learning_rate=learning_rate,\n",
    "                         train=True, verbose=False, tick_s=3600)\n",
    "        \n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset_ideo(cm, session, mini_train_ids_ideo,mini_train_labs_ideo, name=\"Train set\"))\n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset_ideo(cm, session, mini_test_ids_ideo,mini_test_labs_ideo, name=\"Test set\"))\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building pipeline for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# For pretty-printing\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import jinja2\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "HIGHLIGHT_BUTTON_TMPL = jinja2.Template(\"\"\"\n",
    "<script>\n",
    "colors_on = true;\n",
    "function color_cells() {\n",
    "  var ffunc = function(i,e) {return e.innerText {{ filter_cond }}; }\n",
    "  var cells = $('table.dataframe').children('tbody')\n",
    "                                  .children('tr')\n",
    "                                  .children('td')\n",
    "                                  .filter(ffunc);\n",
    "  if (colors_on) {\n",
    "    cells.css('background', 'white');\n",
    "  } else {\n",
    "    cells.css('background', '{{ highlight_color }}');\n",
    "  }\n",
    "  colors_on = !colors_on;\n",
    "}\n",
    "$( document ).ready(color_cells);\n",
    "</script>\n",
    "<form action=\"javascript:color_cells()\">\n",
    "<input type=\"submit\" value=\"Toggle highlighting (val {{ filter_cond }})\"></form>\n",
    "\"\"\")\n",
    "\n",
    "RESIZE_CELLS_TMPL = jinja2.Template(\"\"\"\n",
    "<script>\n",
    "var df = $('table.dataframe');\n",
    "var cells = df.children('tbody').children('tr')\n",
    "                                .children('td');\n",
    "cells.css(\"width\", \"{{ w }}px\").css(\"height\", \"{{ h }}px\");\n",
    "</script>\n",
    "\"\"\")\n",
    "\n",
    "def render_matrix(M, rows=None, cols=None, dtype=float,\n",
    "                        min_size=30, highlight=\"\"):\n",
    "    html = [pd.DataFrame(M, index=rows, columns=cols,\n",
    "                         dtype=dtype)._repr_html_()]\n",
    "    if min_size > 0:\n",
    "        html.append(RESIZE_CELLS_TMPL.render(w=min_size, h=min_size))\n",
    "\n",
    "    if highlight:\n",
    "        html.append(HIGHLIGHT_BUTTON_TMPL.render(filter_cond=highlight,\n",
    "                                             highlight_color=\"yellow\"))\n",
    "\n",
    "    return \"\\n\".join(html)\n",
    "    \n",
    "def pretty_print_matrix(*args, **kwargs):\n",
    "    \"\"\"Pretty-print a matrix using Pandas.\n",
    "    Optionally supports a highlight button, which is a very, very experimental\n",
    "    piece of messy JavaScript. It seems to work for demonstration purposes.\n",
    "    Args:\n",
    "      M : 2D numpy array\n",
    "      rows : list of row labels\n",
    "      cols : list of column labels\n",
    "      dtype : data type (float or int)\n",
    "      min_size : minimum cell size, in pixels\n",
    "      highlight (string): if non-empty, interpreted as a predicate on cell\n",
    "      values, and will render a \"Toggle highlighting\" button.\n",
    "    \"\"\"\n",
    "    html = render_matrix(*args, **kwargs)\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "##\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "##\n",
    "# Data loading functions\n",
    "import nltk\n",
    "import vocabulary\n",
    "\n",
    "def get_corpus(name=\"brown\"):\n",
    "    return nltk.corpus.__getattr__(name)\n",
    "\n",
    "def sents_to_tokens(sents, vocab):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with normal padding.\"\"\"\n",
    "    padded_sentences = ([\"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([canonicalize_word(w, wordset=vocab.wordset)\n",
    "                     for w in flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n",
    "\n",
    "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
    "    \"\"\"Get train and test sentences.\n",
    "    Args:\n",
    "      corpus: nltk.corpus that supports sents() function\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    sentences = np.array(corpus.sents(), dtype=object)\n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "    print (\"Loaded %d sentences (%g tokens)\" % fmt)\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(shuffle)\n",
    "        rng.shuffle(sentences)  # in-place\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(sentences))\n",
    "    train_sentences = sentences[:split_idx]\n",
    "    test_sentences = sentences[split_idx:]\n",
    "\n",
    "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "    print (\"Training set: %d sentences (%d tokens)\" % fmt)\n",
    "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "    print (\"Test set: %d sentences (%d tokens)\" % fmt)\n",
    "\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "def preprocess_sentences(sentences, vocab):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    words = flatten([\"<s> \"] + s + [\" </s>\"] for s in sentences)\n",
    "    words = [canonicalize_word(w, wordset=vocab.word_to_id)\n",
    "             for w in words]\n",
    "    return np.array(vocab.words_to_ids(words))\n",
    "\n",
    "##\n",
    "# Use this function\n",
    "def load_corpus(name, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\"\"\"\n",
    "    corpus = get_corpus(name)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I don't think we actually want to flatten sentences....\n",
    "\n",
    "def sents_to_tokens(sents, vocab):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with normal padding.\"\"\"\n",
    "    padded_sentences = ([\"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([canonicalize_word(w, wordset=vocab.wordset)\n",
    "                     for w in flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    words = []\n",
    "    for i in range(0,corpus.shape[0]):\n",
    "        words += corpus[i].split()\n",
    "    token_feed = (canonicalize_word(w) for w in words)\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n",
    "\n",
    "def get_train_test_sents(corpus, ideo_labs, split=0.8):\n",
    "    \"\"\"Get train and test sentences.\n",
    "    Args:\n",
    "      corpus: nltk.corpus that supports sents() function\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    # Get sentences\n",
    "    sentences = []\n",
    "    for i in range(0,corpus.shape[0]):\n",
    "        sentences.append(corpus[i])\n",
    "        \n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "    print (\"Loaded %d sentences (%g tokens)\" % fmt)\n",
    "\n",
    "    # Split into test and train\n",
    "    train_frac = split\n",
    "    split_idx = int(train_frac * len(sentences))\n",
    "    train_sentences = sentences[:split_idx]\n",
    "    test_sentences = sentences[split_idx:]\n",
    "    \n",
    "    \n",
    "    # Map: Liberal --> (1), Neutral --> (2), Conservative --> (3)\n",
    "    # Map: Liberal --> (1,0,0), Neutral --> (0,1,0), Conservative --> (0,0,1)\n",
    "    labels = []\n",
    "    for i in range(0, ideo_labs.shape[0]):\n",
    "        if ideo_labs[i] == 'Liberal':\n",
    "            labels.append(1.)\n",
    "            #labels.append([1.,0.,0.])\n",
    "        elif ideo_labs[i] == 'Conservative':\n",
    "            labels.append(2.)            \n",
    "            #labels.append([0.,0.,1.])\n",
    "        else:\n",
    "            labels.append(3.)\n",
    "            #labels.append([0.,1.,0.])\n",
    "    labels = np.array(labels)\n",
    "    # Split into test and train\n",
    "    train_labels = labels[:split_idx]\n",
    "    test_labels = labels[split_idx:]\n",
    "            \n",
    "\n",
    "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "    print (\"Training set: %d sentences (%d tokens)\" % fmt)\n",
    "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "    print (\"Test set: %d sentences (%d tokens)\" % fmt)\n",
    "    \n",
    "    return train_sentences, test_sentences, train_labels, test_labels\n",
    "\n",
    "def preprocess_sentences(sentences, vocab):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    flat_sentences = flatten([\"<s> \"] + [s] + [\" </s>\"] for s in sentences)\n",
    "    words = []\n",
    "    for i in range(0, len(flat_sentences)):\n",
    "        words += flat_sentences[i].split()\n",
    "    words = [canonicalize_word(w, wordset=vocab.word_to_id) for w in words]\n",
    "    return np.array(vocab.words_to_ids(words))\n",
    "\n",
    "def process_data(data, labs, split=0.8, V=10000):\n",
    "    \"\"\"Load and split train/test along sentences in dataset.\"\"\"\n",
    "    vocab = build_vocab(data, V)\n",
    "    train_sentences, test_sentences, train_labels, test_labels = get_train_test_sents(data, labs, split=0.8)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(name, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\"\"\"\n",
    "    corpus = get_corpus(name)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(corpus_name, data, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\"\"\"\n",
    "    corpus = get_corpus(name)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brown\n",
    "def build_vocab(corpus, V=10000):\n",
    "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ideo\n",
    "def build_vocab(data, V=10000):\n",
    "    words = []\n",
    "    for i in range(0,data.shape[0]):\n",
    "        words += data[i].split()\n",
    "    token_feed = (canonicalize_word(w) for w in words)\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared_lib import vocabulary_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combined\n",
    "\n",
    "def build_vocab(corpus, data, V=10000):\n",
    "    brown_tokens = (canonicalize_word(w) for w in corpus.words())\n",
    "    \n",
    "    words = []\n",
    "    for i in range(0,data.shape[0]):\n",
    "        words += data[i].split()    \n",
    "    ibc_tokens = (canonicalize_word(w) for w in words)\n",
    "    \n",
    "    vocab = vocabulary_new.CombinedVocabulary(brown_tokens,ibc_tokens, size=V)\n",
    "    \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = get_corpus('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(corpus, data_all, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building an LSTM based off of the CNN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_raw, y = clean_data_and_labels(data_all, labs_all)\n",
    "\n",
    "# Map data into vocabulary\n",
    "max_sentence_len = len(max(data_all, key=len).split())\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_sentence_len)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "\n",
    "# Split up vocabulary\n",
    "split = int(0.9*x.shape[0])\n",
    "x_train, x_dev = x[:split], x[split:]\n",
    "y_train, y_dev = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load_word2vec_format('/Users/megan/Downloads/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating embeddings of pre-trained word vectors\n",
    "\n",
    "# Initialize start, stop, and unk words randomly\n",
    "start = np.random.rand(300,)\n",
    "stop = np.random.rand(300,)\n",
    "unk = np.random.rand(300,)\n",
    "embeddings = np.vstack((start, stop, unk))\n",
    "\n",
    "# Loop through words and pull initialized embeddings\n",
    "for i in range(3, len(vocab.ordered_words())):\n",
    "    try:\n",
    "        vector = model.wv[vocab.ordered_words()[i]]\n",
    "    except KeyError: # the word does not have a pre-initialized vector\n",
    "        vector = np.random.rand(300,) #initialize randomly\n",
    "    \n",
    "    embeddings = np.vstack((embeddings,vector))\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "def matmul3d(X, W):\n",
    "    \"\"\"Wrapper for tf.matmul to handle a 3D input tensor X.\n",
    "    Will perform multiplication along the last dimension.\n",
    "    Args:\n",
    "      X: [m,n,k]\n",
    "      W: [k,l]\n",
    "    Returns:\n",
    "      XW: [m,n,l]\n",
    "    \"\"\"\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)\n",
    "\n",
    "\n",
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(H, forget_bias=0.0)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Defining the graph\n",
    "class initialized_LSTM(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, \n",
    "                 vocab_size,embedding_size, filter_sizes, \n",
    "                 num_filters, embedding):\n",
    "        \n",
    "        # Placeholders for inputs and graph pieces\n",
    "        self.input_w = tf.placeholder(tf.int32, [None, None], name=\"w\")\n",
    "        self.target_y = tf.placeholder(tf.int32, [None, None], name=\"y\")\n",
    "        self.initial_h = None\n",
    "        self.final_h = None\n",
    "        self.logits = None\n",
    "        self.loss = None\n",
    "\n",
    "        # Get dynamic shape info from inputs\n",
    "        with tf.name_scope(\"batch_size\"):\n",
    "            self.batch_size = tf.shape(self.input_w)[0]\n",
    "        with tf.name_scope(\"max_time\"):\n",
    "            self.max_time = tf.shape(self.input_w)[1]\n",
    "\n",
    "        self.ns = tf.tile([self.max_time], [self.batch_size, ], name=\"ns\")\n",
    "    \n",
    "    \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"Embedding_Layer\"):\n",
    "            W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                trainable=True, name=\"W\")\n",
    "            self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_w)\n",
    "            #self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        with tf.name_scope(\"Hidden_Layer\"):\n",
    "            self.cell = MakeFancyRNNCell(embedding_size, keep_prob=0.5, num_layers=3)\n",
    "            self.initial_h = self.cell.zero_state(batch_size = 64, dtype = tf.float32)\n",
    "            self.outputs, self.final_h = tf.nn.dynamic_rnn(cell = self.cell, inputs=self.embedded_chars,\n",
    "                                                           initial_state = self.initial_h, \n",
    "                                                           sequence_length = self.ns)\n",
    "        \n",
    "\n",
    "        with tf.name_scope(\"Output_Layer\"):\n",
    "            self.b_out = tf.Variable(tf.zeros([num_classes,], dtype=tf.float32), name=\"b_out\")\n",
    "            self.W_out = tf.Variable(tf.random_uniform([embedding_size,num_classes],0,1.0), name=\"W_out\")\n",
    "            # Calculate logits\n",
    "            self.logits = tf.add(matmul3d(self.outputs, self.W_out), self.b_out, name=\"logits\")\n",
    "    \n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "        # Define outputs\n",
    "        with tf.name_scope(\"Output_Layer\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"Cost_Function\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        \n",
    "        # Calculate Accuracy to compare to other models\n",
    "        with tf.name_scope(\"Accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the graph\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = initialized_CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=3,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=300,\n",
    "            filter_sizes=map(int, '3,4,5'.split(\",\")),\n",
    "            num_filters=128,\n",
    "            embedding = embeddings\n",
    "        )\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: embeddings})\n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining an epoch\n",
    "def train_epoch(x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training epoch\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 0.5\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    #time_str = datetime.datetime.now().isoformat()\n",
    "    #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "def dev_epoch(x_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy, predictions = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate batches\n",
    "def batch_generator(data, labels, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index], labels[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate batches\n",
    "batches = batch_generator(x_train, y_train, batch_size = 64, num_epochs = 5)\n",
    "\n",
    "# Run model with a training loop\n",
    "for batch in batches:\n",
    "    x_batch, y_batch = batch\n",
    "    train_epoch(x_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % 50 == 0: # evaluate every 50 steps\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_epoch(x_dev, y_dev, writer=dev_summary_writer)\n",
    "        print(\"\")\n",
    "    if current_step % 100 == 0: # checkpoint every 100 steps\n",
    "        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "        print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
