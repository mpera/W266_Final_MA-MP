{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a simpler CNN classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Massaging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "\n",
    "from shared_lib import vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and save data into liberal, conservative and neutral objects\n",
    "[lib, con, neutral] = pickle.load(open('ibcData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Formatting data into workable arrays\n",
    "liberal = np.array(lib)\n",
    "conserv = np.array(con)\n",
    "neut = np.array(neutral)\n",
    "\n",
    "# Seprating data and labels\n",
    "def separate_data_and_labels(label_class):\n",
    "    labels = []\n",
    "    data = []\n",
    "    for i in range(len(label_class)):\n",
    "        for node in label_class[i]:\n",
    "            if hasattr(node, 'label'):\n",
    "                data.append(node.get_words())\n",
    "                labels.append(node.label)\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "lib_data, lib_labs = separate_data_and_labels(liberal)\n",
    "con_data, con_labs = separate_data_and_labels(conserv)\n",
    "neut_data, neut_labs = separate_data_and_labels(neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621,)\n",
      "(22621,)\n"
     ]
    }
   ],
   "source": [
    "# Combining into one dataset\n",
    "data_all = np.concatenate((neut_data, lib_data, con_data), axis=0)\n",
    "labs_all = np.concatenate((neut_labs, lib_labs, con_labs), axis=0)\n",
    "\n",
    "print (data_all.shape)\n",
    "print (labs_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly mixing data&labels so that they can be split into test and train\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "data_all, labs_all = shuffle_in_unison(data_all, labs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621,)\n",
      "(4525,)\n",
      "(18096,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into test (20%) and train (80%)\n",
    "slice = int(.8*labs_all.shape[0])\n",
    "data_train = data_all[:slice]\n",
    "labs_train = labs_all[:slice]\n",
    "data_test = data_all[slice:]\n",
    "labs_test = labs_all[slice:]\n",
    "print(labs_all.shape)\n",
    "print(labs_test.shape)\n",
    "print(labs_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def clean_data_and_labels(sentences, labels):\n",
    "    \"\"\"\n",
    "    Takes an array of sentences and their labels.\n",
    "    Splits the data into words and generates labels. Returns clean sentences and labels.\n",
    "    \"\"\"\n",
    "    # Array to list of sentences\n",
    "    x_text = [s.strip() for s in sentences]\n",
    "    # Clean words\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    \n",
    "    # Generate labels\n",
    "    # Map: Liberal --> (1,0,0), Neutral --> (0,1,0), Conservative --> (0,0,1)\n",
    "    y = []\n",
    "    for i in range(0, labels.shape[0]):\n",
    "        if labels[i] == 'Liberal':\n",
    "            y.append([1,0,0])\n",
    "        elif labels[i] == 'Conservative':           \n",
    "            y.append([0,0,1])\n",
    "        else:\n",
    "            y.append([0,1,0])\n",
    "            \n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    words = []\n",
    "    for i in range(0,corpus.shape[0]):\n",
    "        words += corpus[i].split()\n",
    "    token_feed = (canonicalize_word(w) for w in words)\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(data_all, V = 14836)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Working CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Defining the graph\n",
    "class CNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,embedding_size, filter_sizes, num_filters):\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"Embedding_Layer\"):\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Max-pooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * 3 #len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "        # Define outputs\n",
    "        with tf.name_scope(\"Output_Layer\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"Cost_Function\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        \n",
    "        # Calculate Accuracy to compare to other models\n",
    "        with tf.name_scope(\"Accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_raw, y = clean_data_and_labels(data_all, labs_all)\n",
    "\n",
    "# Map data into vocabulary\n",
    "max_sentence_len = len(max(data_all, key=len).split())\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_sentence_len)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "\n",
    "# Split up vocabulary\n",
    "split = int(0.9*x.shape[0])\n",
    "x_train, x_dev = x[:split], x[split:]\n",
    "y_train, y_dev = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the graph\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=3,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=128,\n",
    "            filter_sizes=map(int, '3,4,5'.split(\",\")),\n",
    "            num_filters=128)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining an epoch\n",
    "def train_epoch(x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training epoch\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 0.5\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    #time_str = datetime.datetime.now().isoformat()\n",
    "    #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "def dev_epoch(x_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy, predictions = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate batches\n",
    "def batch_generator(data, labels, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index], labels[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate batches\n",
    "batches = batch_generator(x_train, y_train, batch_size = 64, num_epochs = 5)\n",
    "\n",
    "# Training loop\n",
    "for batch in batches:\n",
    "    x_batch, y_batch = batch\n",
    "    train_epoch(x_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % 50 == 0: # evaluate every 50 steps\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_epoch(x_dev, y_dev, writer=dev_summary_writer)\n",
    "        print(\"\")\n",
    "    if current_step % 100 == 0: # checkpoint every 100 steps\n",
    "        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "        print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Showing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting predictions for the dev set\n",
    "predictions = dev_epoch(x_dev, y_dev, writer=dev_summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Printing out examples of predictions in the dev set\n",
    "data_dev = data_all[split:]\n",
    "\n",
    "print(\"Note:\",'\\n',\"Map: Liberal --> 0, Neutral --> 1, Conservative --> 2\",'\\n')\n",
    "for i in range(0, 10):\n",
    "    correct_label = y_dev[i].index(max(y_dev[i]))\n",
    "    if predictions[i] == correct_label:\n",
    "        print(\"CORRECT:\")\n",
    "        print(\"Correct Label:\", correct_label)\n",
    "        print (\"Predicted Label:\", predictions[i])\n",
    "        print(data_dev[i], '\\n')\n",
    "    else:\n",
    "        print(\"WRONG:\")\n",
    "        print(\"Correct Label:\", correct_label)\n",
    "        print (\"Predicted Label:\", predictions[i])\n",
    "        print(data_dev[i], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Google Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.mattmahoney.net/dc/textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load_word2vec_format('/Users/megan/Downloads/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating embeddings of pre-trained word vectors\n",
    "\n",
    "# Initialize start, stop, and unk words randomly\n",
    "start = np.random.rand(300,)\n",
    "stop = np.random.rand(300,)\n",
    "unk = np.random.rand(300,)\n",
    "embeddings = np.vstack((start, stop, unk))\n",
    "\n",
    "# Loop through words and pull initialized embeddings\n",
    "for i in range(3, len(vocab.ordered_words())):\n",
    "    try:\n",
    "        vector = model.wv[vocab.ordered_words()[i]]\n",
    "    except KeyError: # the word does not have a pre-initialized vector\n",
    "        vector = np.random.rand(300,) #initialize randomly\n",
    "    \n",
    "    embeddings = np.vstack((embeddings,vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14836, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Defining the graph\n",
    "class initialized_CNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, \n",
    "                 vocab_size,embedding_size, filter_sizes, \n",
    "                 num_filters, embedding):\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"Embedding_Layer\"):\n",
    "            W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                trainable=True, name=\"W\")\n",
    "            self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Max-pooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * 3 #len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "        # Define outputs\n",
    "        with tf.name_scope(\"Output_Layer\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"Cost_Function\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        \n",
    "        # Calculate Accuracy to compare to other models\n",
    "        with tf.name_scope(\"Accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/megan/Documents/W266_final_project/runs/1493485591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building the graph\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = initialized_CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=3,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=300,\n",
    "            filter_sizes=map(int, '3,4,5'.split(\",\")),\n",
    "            num_filters=128,\n",
    "            embedding = embeddings\n",
    "        )\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: embeddings})\n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:07:23.973549: step 50, loss 1.04479, acc 0.542201\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:08:08.546536: step 100, loss 0.991615, acc 0.542643\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:08:53.087333: step 150, loss 0.950423, acc 0.539991\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:09:39.640344: step 200, loss 0.924513, acc 0.548829\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:10:27.950290: step 250, loss 0.905098, acc 0.571807\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:11:15.136827: step 300, loss 0.880652, acc 0.601414\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:12:03.063064: step 350, loss 0.858892, acc 0.655767\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:12:50.893534: step 400, loss 0.837894, acc 0.693327\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:13:38.469853: step 450, loss 0.807216, acc 0.694211\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:14:24.435617: step 500, loss 0.775685, acc 0.69863\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:15:14.678816: step 550, loss 0.749147, acc 0.712329\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:16:01.485149: step 600, loss 0.724064, acc 0.719841\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:16:48.823638: step 650, loss 0.699415, acc 0.732656\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:17:38.599964: step 700, loss 0.675701, acc 0.746354\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:18:35.124036: step 750, loss 0.653167, acc 0.763146\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:19:26.330652: step 800, loss 0.629779, acc 0.775077\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:20:19.746550: step 850, loss 0.610457, acc 0.7711\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:21:08.060370: step 900, loss 0.59588, acc 0.781264\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:21:53.265904: step 950, loss 0.578356, acc 0.792753\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:22:43.417182: step 1000, loss 0.563541, acc 0.800265\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:23:35.316650: step 1050, loss 0.553035, acc 0.802033\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:24:38.800992: step 1100, loss 0.534482, acc 0.813964\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:25:59.539259: step 1150, loss 0.522446, acc 0.817057\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:27:06.155252: step 1200, loss 0.516259, acc 0.82015\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:28:13.046794: step 1250, loss 0.504489, acc 0.82236\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:29:24.004275: step 1300, loss 0.499251, acc 0.825895\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:30:30.818733: step 1350, loss 0.491933, acc 0.833407\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:31:28.966119: step 1400, loss 0.480943, acc 0.836058\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:32:37.756966: step 1450, loss 0.472931, acc 0.8365\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:33:59.737041: step 1500, loss 0.473871, acc 0.835175\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_final_project/runs/1493485591/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T10:35:08.918517: step 1550, loss 0.466486, acc 0.837384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate batches\n",
    "batches = batch_generator(x_train, y_train, batch_size = 64, num_epochs = 5)\n",
    "\n",
    "# Training loop\n",
    "for batch in batches:\n",
    "    x_batch, y_batch = batch\n",
    "    train_epoch(x_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % 50 == 0: # evaluate every 50 steps\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_epoch(x_dev, y_dev, writer=dev_summary_writer)\n",
    "        print(\"\")\n",
    "    if current_step % 100 == 0: # checkpoint every 100 steps\n",
    "        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "        print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-29T10:49:33.657669: step 1595, loss 0.461783, acc 0.836942\n",
      "Note: \n",
      " Map: Liberal --> 0, Neutral --> 1, Conservative --> 2 \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 0\n",
      "Predicted Label: 0\n",
      "a global agreement on a new ( strategic plan to halt biodiversity loss ) , the mobilisation of the finance needed to make it happen and a new legally-binding protocol on access and benefit sharing ( ABS ) \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 0\n",
      "Predicted Label: 0\n",
      "a summer replacement sitcom created by Alan King , depicted impoverished Muscovites suffering deprivations and corny bread line jokes while living in a cramped one-bedroom apartment \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "the housing market \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "three days of oral arguments \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 2\n",
      "Predicted Label: 2\n",
      "the Libertarian Party 's Bob Barr , who had an excellent immigration record as a Republican congressman and who has not totally capitulated to the culturally illiterate left-libertarianism that now dominates the movement after the tragic demise of Murray Rothbard and paleolibertarianism \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "various sorts \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "the cultural , as opposed to economic , consequences \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "the public good of wealth \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "his street vendor 's \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 0\n",
      "Predicted Label: 0\n",
      "I urged him to use his influence to protest government-created residential segregation that concentrates the most disadvantaged children in schools without middleclass peers and where the accumulation of impediments to learning is overwhelming . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions for the dev set\n",
    "predictions = dev_epoch(x_dev, y_dev, writer=dev_summary_writer)\n",
    "\n",
    "# Printing out examples of predictions in the dev set\n",
    "data_dev = data_all[split:]\n",
    "\n",
    "print(\"Note:\",'\\n',\"Map: Liberal --> 0, Neutral --> 1, Conservative --> 2\",'\\n')\n",
    "for i in range(0, 10):\n",
    "    correct_label = y_dev[i].index(max(y_dev[i]))\n",
    "    if predictions[i] == correct_label:\n",
    "        print(\"CORRECT:\")\n",
    "        print(\"Correct Label:\", correct_label)\n",
    "        print (\"Predicted Label:\", predictions[i])\n",
    "        print(data_dev[i], '\\n')\n",
    "    else:\n",
    "        print(\"WRONG:\")\n",
    "        print(\"Correct Label:\", correct_label)\n",
    "        print (\"Predicted Label:\", predictions[i])\n",
    "        print(data_dev[i], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
