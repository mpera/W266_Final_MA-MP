{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM Sample code\n",
    "# from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import collections, itertools\n",
    "import re\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import vocabulary, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "def sents_to_tokens(sents, vocab):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with normal padding.\"\"\"\n",
    "    padded_sentences = ([\"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([canonicalize_word(w, wordset=vocab.wordset)\n",
    "                     for w in flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    words = []\n",
    "    for i in range(0,corpus.shape[0]):\n",
    "        words += corpus[i].split()\n",
    "    token_feed = (canonicalize_word(w) for w in words)\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def preprocess_sentences(corpus, vocab):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Get sentences\n",
    "    sents = []\n",
    "    for i in range(0,corpus.shape[0]):\n",
    "        sents.append(corpus[i])\n",
    "    \n",
    "    # Map sentences to word ids after canonicalizing\n",
    "    sentences = []\n",
    "    for i in range(0, len(sents)):\n",
    "        words = sents[i].split()\n",
    "        words = [canonicalize_word(w, wordset=vocab.word_to_id) for w in words]\n",
    "        word_ids = vocab.words_to_ids(words)\n",
    "        sentences.append(word_ids)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def process_data(data, V=10000):\n",
    "    \"\"\"Load and split train/test along sentences in dataset.\"\"\"\n",
    "    corpus = data\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    sentences = preprocess_sentences(corpus, vocab)\n",
    "    return vocab, sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "vocab, processed_data_train = process_data(data_train)\n",
    "vocab, processed_data_test = process_data(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find max_seq_length from processed data\n",
    "max_sequence = max(len(max(processed_data_test,key=len)), len(max(processed_data_train,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataHandling(object):\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data, labs, max_seq_len=max_sequence):\n",
    "        self.data = data\n",
    "        ideo_labs = labs\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        self.batch_id = 0\n",
    "\n",
    "        # Make inner lists\n",
    "        for i in range(0, len(self.data)):\n",
    "            new_list = [[self.data[i][x]] for x in range(0, len(self.data[i]))]\n",
    "            self.data[i] = new_list\n",
    "        \n",
    "        # Pad sequence for dimension consistency\n",
    "        for i in range(0, len(self.data)):\n",
    "            if len(self.data[i]) < max_seq_len:\n",
    "                s = []\n",
    "                s += [[0.] for i in range(max_seq_len - len(self.data[i]))]\n",
    "                self.data[i] += s\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Map: Liberal --> (1,0,0), Neutral --> (0,1,0), Conservative --> (0,0,1)\n",
    "        for i in range(0, ideo_labs.shape[0]):\n",
    "            if ideo_labs[i] == 'Liberal':\n",
    "                self.labels.append([1.,0.,0.])\n",
    "            elif ideo_labs[i] == 'Conservative':\n",
    "                self.labels.append([0.,0.,1.])\n",
    "            else:\n",
    "                self.labels.append([0.,1.,0.])\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data to pass to LSTM\n",
    "trainset = DataHandling(processed_data_train,labs_train)\n",
    "testset = DataHandling(processed_data_test,labs_test)\n",
    "\n",
    "# Show train/test data and labels are the same shape\n",
    "print('Train data shape: ', len(trainset.data))\n",
    "print('Train label shape: ', len(trainset.labels))\n",
    "print('Test data shape: ', len(testset.data))\n",
    "print('Test label shape: ', len(testset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 1000000\n",
    "batch_size = 20\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = max_sequence # Sequence max length\n",
    "n_hidden = 64 # hidden layer num of features\n",
    "n_classes = 3 # linear sequence or not\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n",
    "                                             seqlen: batch_seqlen})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
