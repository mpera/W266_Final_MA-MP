{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project\n",
    "\n",
    "### Classifying the Political Ideology of News Articles\n",
    "\n",
    "#### Matt Accociamessa and Megan Pera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megan/anaconda/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/megan/anaconda/lib/python3.4/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import genderal libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import scipy as sp\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Libraries for Baseline MNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Libraries for CLIP model\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Libraries for LSTM/CNN\n",
    "from shared_lib import vocabulary, utils, utils_ideo\n",
    "import rnnlm\n",
    "import tensorflow as tf\n",
    "from gensim.models import word2vec\n",
    "import json, shutil, sys\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Libraries for article scraping\n",
    "import string\n",
    "import urllib.request\n",
    "import httplib2\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError, RequestException\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Cleaning and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load and save data into liberal, conservative and neutral objects\n",
    "[lib, con, neutral] = pickle.load(open('ibcData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal examples (out of  2025  sentences): \n",
      "Forcing middle-class workers to bear a greater share of the cost of government weakens their support for needed investments and stirs resentment toward those who depend on public services the most .\n",
      "Because it would not be worthwhile to bring a case for $ 30.22 , the arbitration clause would , as a practical matter , deny the Concepcions any relief and , more important , eliminate a class action that might punish AT&T for its pattern of fraudulent behavior .\n",
      "Indeed , Lind argues that high profits and high wages reinforce each other because workers then have the wherewithal to buy the products they are making .\n",
      "In fairness , it should be noted that he devotes an entire chapter to New York Times political columnist Maureen Dowd , a liberal who makes much of the outsized rivalries , jealousies , and personalities that dominate American politics .\n",
      "Psychological tactics are social control techniques that operate at the level of the mind , with the goal of creating fear and making it difficult for protesters to successfully mobilize .\n",
      "\n",
      "Conservative examples (out of  1701  sentences): \n",
      "Gore is getting rich from environmentalism , not just by being paid a whopping $ 175,000 per speech but by using political pressure to force government policy in a direction that benefits his business interests .\n",
      "The Federal Housing Finance Regulatory Reform Act of 2008 should have been an easy sell , since it purportedly aimed to assist homeowners , a more popular ( or at least more sentimentalized ) subset of Americans than greedy Wall Street tycoons .\n",
      "Yet for all its submerged class snobbery and anti-intellectualism disguised as cool detachment , the ultimate failure of the Washington media lies less with the personal failings of its elite members than its structural inadequacy .\n",
      "Rumsfeld then went on to discuss how China 's lack of transparency with respect to its defense expenditures and activities raises doubts in the region about China 's intentions .\n",
      "You never hear from the co-conspirators of the left-wing media how many innocent victims are dead , raped , and mutilated as a direct result of these left-wing policies and insane anti-gun laws .\n",
      "\n",
      "Neutral examples (out of  600  sentences): \n",
      "In this country , the beneficiaries of Apple 's success are , first , the designers , who have done wonders working with Steve Jobs to produce products that are beautiful and effective .\n",
      "The problem with this argument is that China reports about 68 percent of the world 's aquaculture production , and the FAO , which has been burned by inflated Chinese statistics before , expresses doubt about its stated production and growth rates . ''\n",
      "The soil exhaustion caused by the plantation system , as well as the relatively low productivity of forced labor , compelled planters to seek new lands to exploit .\n",
      "The same complexity that leads to such malfunctions also creates vulnerabilities that human agents can use to make computer systems operate in unintended ways .\n",
      "Threads of new awkwardness stretch out between them , and nature itself winks behind their backs and plays nasty tricks on them , scattering yellow clods of asters and groundsel , blanketing purple clover and pink flax , erecting stalks of huge -- but smelly -- purple arum flowers , sprinkling red buttercups , and hanging baby oranges and lemons on the trees around them .\n"
     ]
    }
   ],
   "source": [
    "# Data samples, by classification\n",
    "print ('Liberal examples (out of ', len(lib), ' sentences): ')\n",
    "for tree in lib[0:5]:\n",
    "    print(tree.get_words())\n",
    "    \n",
    "print ('\\nConservative examples (out of ', len(con), ' sentences): ')\n",
    "for tree in con[0:5]:\n",
    "    print (tree.get_words())\n",
    "    \n",
    "print ('\\nNeutral examples (out of ', len(neutral), ' sentences): ')\n",
    "for tree in neutral[0:5]:\n",
    "    print (tree.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Formatting data into workable arrays\n",
    "liberal = np.array(lib)\n",
    "conserv = np.array(con)\n",
    "neut = np.array(neutral)\n",
    "\n",
    "# Seprating data and labels\n",
    "def separate_data_and_labels(label_class):\n",
    "    labels = []\n",
    "    data = []\n",
    "    for i in range(len(label_class)):\n",
    "        for node in label_class[i]:\n",
    "            if hasattr(node, 'label'):\n",
    "                data.append(node.get_words())\n",
    "                labels.append(node.label)\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "lib_data, lib_labs = separate_data_and_labels(liberal)\n",
    "con_data, con_labs = separate_data_and_labels(conserv)\n",
    "neut_data, neut_labs = separate_data_and_labels(neut)\n",
    "conmult = (len(lib_data))/(len(con_data))\n",
    "neutmult = (len(lib_data))/(len(neut_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\n",
      " Liberal\n",
      "Forcing middle-class workers to bear a greater share of the cost of government weakens their support for needed investments and stirs resentment toward those who depend on public services the most . \n",
      " ['Liberal' 'Liberal' 'Liberal' 'Liberal' 'Liberal' 'Liberal' 'Liberal'\n",
      " 'Liberal' 'Liberal' 'Liberal']\n",
      "\n",
      " Conservative\n",
      "Gore is getting rich from environmentalism , not just by being paid a whopping $ 175,000 per speech but by using political pressure to force government policy in a direction that benefits his business interests . \n",
      " ['Conservative' 'Conservative' 'Conservative' 'Conservative' 'Neutral'\n",
      " 'Neutral' 'Neutral' 'Conservative' 'Liberal' 'Liberal']\n",
      "\n",
      " Neutral\n",
      "In this country , the beneficiaries of Apple 's success are , first , the designers , who have done wonders working with Steve Jobs to produce products that are beautiful and effective . \n",
      " ['Neutral' 'Neutral' 'Neutral' 'Neutral' 'Neutral' 'Neutral' 'Neutral'\n",
      " 'Neutral' 'Neutral' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "print('Examples:')\n",
    "print ('\\n Liberal')\n",
    "print(lib_data[0],'\\n',lib_labs[0:10])\n",
    "print ('\\n Conservative')\n",
    "print(con_data[0],'\\n',con_labs[0:10])\n",
    "print ('\\n Neutral')\n",
    "print(neut_data[0],'\\n',neut_labs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621,)\n",
      "(22621,)\n"
     ]
    }
   ],
   "source": [
    "# Combining into one dataset\n",
    "data_all = np.concatenate((neut_data, lib_data, con_data), axis=0)\n",
    "labs_all = np.concatenate((neut_labs, lib_labs, con_labs), axis=0)\n",
    "\n",
    "print (data_all.shape)\n",
    "print (labs_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly mixing data&labels so that they can be split into test and train\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "data_all, labs_all = shuffle_in_unison(data_all, labs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621,)\n",
      "(4525,)\n",
      "(18096,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into test (20%) and train (80%)\n",
    "slice = int(.8*labs_all.shape[0])\n",
    "data_train = data_all[:slice]\n",
    "labs_train = labs_all[:slice]\n",
    "data_test = data_all[slice:]\n",
    "labs_test = labs_all[slice:]\n",
    "print(labs_all.shape)\n",
    "print(labs_test.shape)\n",
    "print(labs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 309335\n",
      "Vocabulary size: 14127\n"
     ]
    }
   ],
   "source": [
    "# Turning dataset into word tokens\n",
    "count_vect = CountVectorizer()\n",
    "data = count_vect.fit_transform(data_train).toarray()\n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# Counting the number of times each word appears\n",
    "np.clip(data,0,1, out = data) #make sure each word only appears once in the array\n",
    "dist = np.sum(data, axis = 0) #sum the columns\n",
    "counts = list(zip(vocab,dist)) #zip counts and words together\n",
    "\n",
    "# Total vocab size and word count\n",
    "print(\"Total word count:\",np.sum(dist))\n",
    "print(\"Vocabulary size:\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 10985),\n",
       " ('and', 7948),\n",
       " ('of', 7903),\n",
       " ('to', 7616),\n",
       " ('that', 4746),\n",
       " ('in', 4545),\n",
       " ('for', 3462),\n",
       " ('by', 2287),\n",
       " ('is', 2207),\n",
       " ('on', 2043)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the 10 most popular words\n",
    "counts = sorted(counts, key=lambda x: x[1], reverse=True) \n",
    "counts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3UAAAGiCAYAAABAqu+eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZoCxU2eMMcYYY4wxA8z/A9cuSCz9RILzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137ab3eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting top 50 results\n",
    "ordered = list(zip(*counts))\n",
    "x = ordered[0][:50] #counts\n",
    "y = ordered[1][:50] #words\n",
    "\n",
    "# Plotting figure\n",
    "fig = plt.figure(figsize=(15.0,6.0))\n",
    "indexes = np.arange(50)\n",
    "width = .5\n",
    "plt.bar(indexes, y, width)\n",
    "plt.xticks(indexes + width * 0.5, x,rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial Naive Bayes:\n",
      "alpha: 0.0001 F1: 0.73546961326\n",
      "alpha: 0.01 F1: 0.731933701657\n",
      "alpha: 0.05 F1: 0.730828729282\n",
      "alpha: 0.1 F1: 0.725524861878\n",
      "alpha: 0.2 F1: 0.72044198895\n",
      "alpha: 1.0 F1: 0.702541436464\n"
     ]
    }
   ],
   "source": [
    "# This model predicts the political leanings of sentences and sub-sentences\n",
    "\n",
    "# Training the model\n",
    "vect = CountVectorizer()\n",
    "train_vocab = vect.fit_transform(data_train)\n",
    "test_vocab = vect.transform(data_test)\n",
    "\n",
    "# Scoring the model\n",
    "print(\"\")\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "for a in [0.0001, 0.01, .05, 0.1, 0.2, 1.0]:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train_vocab, labs_train)\n",
    "    mnbpreds = mnb.predict(test_vocab)\n",
    "    print(\"alpha:\", a, \"F1:\", mnb.score(test_vocab, labs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: an alliance between the public groups that define the need for regulation and those parts of business that make it a success\n",
      "Actual Label: Liberal\n",
      "Predicted Label: Liberal\n",
      "Predicted Label Probability: 0.9907203988545072 \n",
      "\n",
      "Sentence: the economy as a whole\n",
      "Actual Label: Neutral\n",
      "Predicted Label: Neutral\n",
      "Predicted Label Probability: 0.6058666778884277 \n",
      "\n",
      "Sentence: spent on the program as spending rages out of control\n",
      "Actual Label: Conservative\n",
      "Predicted Label: Conservative\n",
      "Predicted Label Probability: 0.9999416910055227 \n",
      "\n",
      "Sentence: easy access to our country\n",
      "Actual Label: Neutral\n",
      "Predicted Label: Liberal\n",
      "Predicted Label Probability: 0.4811325108987772 \n",
      "\n",
      "Sentence: been designated high-risk targets for terrorism , because that means more dollars , more jobs , more shiny riot gear ...\n",
      "Actual Label: Conservative\n",
      "Predicted Label: Conservative\n",
      "Predicted Label Probability: 0.999999999980588 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing examples for alpha = 0.001\n",
    "mnb = MultinomialNB(alpha=0.001)\n",
    "mnb.fit(train_vocab, labs_train)\n",
    "mnbpreds = mnb.predict(test_vocab)\n",
    "mnbpred_prob = mnb.predict_proba(test_vocab)\n",
    "probs = list(zip(data_test.tolist(),mnbpreds.tolist(),mnbpred_prob.tolist()))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print('Sentence:',probs[i][0])\n",
    "    print('Actual Label:',labs_test[i])\n",
    "    print('Predicted Label:',probs[i][1])\n",
    "    print('Predicted Label Probability:', max(probs[i][2]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probability error analysis\n",
    "errors = []\n",
    "correct_probs = []\n",
    "incorrect_probs = []\n",
    "for i in range(0,len(probs)):\n",
    "    if labs_test[i] == probs[i][1]:\n",
    "        correct_probs.append(max(probs[i][2]))\n",
    "    else:\n",
    "        errors.append(i)\n",
    "        incorrect_probs.append(max(probs[i][2]))\n",
    "\n",
    "# Mislabels error analysis\n",
    "correct_labels = defaultdict(int)\n",
    "con_as_neut = 0\n",
    "con_as_lib = 0\n",
    "lib_as_neut = 0\n",
    "lib_as_con = 0\n",
    "neut_as_con = 0\n",
    "neut_as_lib = 0\n",
    "\n",
    "for i in range(0,len(probs)):\n",
    "    if labs_test[i] == probs[i][1]:\n",
    "        correct_labels[labs_test[i]] += 1\n",
    "    else:\n",
    "        if labs_test[i] == 'Conservative' and probs[i][1] == 'Neutral':\n",
    "            con_as_neut += 1\n",
    "        elif labs_test[i] == 'Conservative' and probs[i][1] == 'Liberal':\n",
    "            con_as_lib += 1\n",
    "        elif labs_test[i] == 'Neutral' and probs[i][1] == 'Liberal':\n",
    "            neut_as_lib += 1\n",
    "        elif labs_test[i] == 'Neutral' and probs[i][1] == 'Conservative':\n",
    "            neut_as_con += 1\n",
    "        elif labs_test[i] == 'Liberal' and probs[i][1] == 'Neutral':\n",
    "            lib_as_neut += 1\n",
    "        elif labs_test[i] == 'Liberal' and probs[i][1] == 'Conservative':\n",
    "            lib_as_con += 1\n",
    "        \n",
    "# Getting overall phrase label distribution\n",
    "labels = defaultdict(int)\n",
    "for i in range(labs_test.shape[0]):\n",
    "    labels[labs_test[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrectly labeled Liberal labels: 292\n",
      "Number of incorrectly labeled Neutral labels: 623\n",
      "Number of incorrectly labeled Conservative labels: 290\n"
     ]
    }
   ],
   "source": [
    "# Comparing overall distribution of sentences to correctly labeled sentences\n",
    "for key in labels:\n",
    "    print('Number of incorrectly labeled', key, 'labels:',labels[key] - correct_labels[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conservative phrases labeled as neutral: 75\n",
      "Number of conservative phrases labeled as liberal: 215\n",
      "Number of liberal phrases labeled as neutral: 107\n",
      "Number of liberal phrases labeled as conservative: 185\n",
      "Number of neutral phrases labeled as conservative: 260\n",
      "Number of neutral phrases labeled as liberal: 363\n"
     ]
    }
   ],
   "source": [
    "print('Number of conservative phrases labeled as neutral:',con_as_neut)\n",
    "print('Number of conservative phrases labeled as liberal:',con_as_lib)\n",
    "print('Number of liberal phrases labeled as neutral:',lib_as_neut)\n",
    "print('Number of liberal phrases labeled as conservative:',lib_as_con)\n",
    "print('Number of neutral phrases labeled as conservative:',neut_as_con)\n",
    "print('Number of neutral phrases labeled as liberal:',neut_as_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB missclassified 1205 phrases \n",
      "\n",
      "Sentence: easy access to our country\n",
      "Actual Label: Neutral\n",
      "Predicted Label: Liberal\n",
      "Predicted Label Probability: 0.4811325108987772 \n",
      "\n",
      "Sentence: Criminalizing abortion would not , for instance , address the needs of women who seek an abortion because they lacked contraception or were raped or are living in abusive relationship , or will have to drop out of work or school to raise a child alone , or are stretched so thin that they can not emotionally or financially provide for their other children .\n",
      "Actual Label: Liberal\n",
      "Predicted Label: Conservative\n",
      "Predicted Label Probability: 0.5638281155536243 \n",
      "\n",
      "Sentence: been the preferred solution of the right , which has argued for a move toward high-deductible care , in which individuals bear more financial risk and vulnerability\n",
      "Actual Label: Liberal\n",
      "Predicted Label: Conservative\n",
      "Predicted Label Probability: 0.8534489604720796 \n",
      "\n",
      "Sentence: have been made in the system of subsidies for agrofuels\n",
      "Actual Label: Neutral\n",
      "Predicted Label: Liberal\n",
      "Predicted Label Probability: 0.6827384914439372 \n",
      "\n",
      "Sentence: This has been the preferred solution of the right , which has argued for a move toward high-deductible care , in which individuals bear more financial risk and vulnerability .\n",
      "Actual Label: Liberal\n",
      "Predicted Label: Conservative\n",
      "Predicted Label Probability: 0.8971322958273361 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing out examples of mistakes\n",
    "print('MNB missclassified',len(errors),'phrases','\\n')\n",
    "\n",
    "for i in errors[0:5]:\n",
    "    print('Sentence:',probs[i][0])\n",
    "    print('Actual Label:',labs_test[i])\n",
    "    print('Predicted Label:',probs[i][1])\n",
    "    print('Predicted Label Probability:', max(probs[i][2]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning dataset into word tokens for split sets\n",
    "\n",
    "# Lib data\n",
    "count_vect = CountVectorizer()\n",
    "data = count_vect.fit_transform(lib_data).toarray()\n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# Counting the number of times each word appears\n",
    "np.clip(data,0,1, out = data) #make sure each word only appears once in the array\n",
    "dist = np.sum(data, axis = 0) #sum the columns\n",
    "counts = list(zip(vocab,dist)) #zip counts and words together\n",
    "counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "ordereda = list(zip(*counts))\n",
    "xxa = ordereda[0] #words\n",
    "yya = ordereda[1] #counts\n",
    "libdict = dict()\n",
    "for i in range(len(xxa)):\n",
    "    libdict.update({xxa[i]: yya[i]})\n",
    "\n",
    "# Con data\n",
    "count_vect = CountVectorizer()\n",
    "data = count_vect.fit_transform(con_data).toarray()\n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# Counting the number of times each word appears\n",
    "np.clip(data,0,1, out = data) #make sure each word only appears once in the array\n",
    "dist = np.sum(data, axis = 0) #sum the columns\n",
    "counts = list(zip(vocab,dist)) #zip counts and words together\n",
    "counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "orderedb = list(zip(*counts))\n",
    "xxb = orderedb[0] #words\n",
    "yyb = orderedb[1] #counts\n",
    "condict = dict()\n",
    "for i in range(len(xxb)):\n",
    "    condict.update({xxb[i]: yyb[i]})\n",
    "\n",
    "# Neut data\n",
    "count_vect = CountVectorizer()\n",
    "data = count_vect.fit_transform(neut_data).toarray()\n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# Counting the number of times each word appears\n",
    "np.clip(data,0,1, out = data) #make sure each word only appears once in the array\n",
    "dist = np.sum(data, axis = 0) #sum the columns\n",
    "counts = list(zip(vocab,dist)) #zip counts and words together\n",
    "counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "orderedc = list(zip(*counts))\n",
    "xxc = orderedc[0] #words\n",
    "yyc = orderedc[1] #counts\n",
    "neutdict = dict()\n",
    "for i in range(len(xxc)):\n",
    "    neutdict.update({xxc[i]: yyc[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lib</th>\n",
       "      <th>con</th>\n",
       "      <th>neut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>64</td>\n",
       "      <td>31.992188</td>\n",
       "      <td>62.239954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>093</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>26.660156</td>\n",
       "      <td>28.007979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>23</td>\n",
       "      <td>13.330078</td>\n",
       "      <td>43.567968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.559989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>10.664062</td>\n",
       "      <td>31.119977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>21.328125</td>\n",
       "      <td>3.111998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>3.999023</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>5.332031</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14th</th>\n",
       "      <td>0</td>\n",
       "      <td>2.666016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>10.664062</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>7</td>\n",
       "      <td>13.330078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>10.664062</td>\n",
       "      <td>15.559989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "      <td>9.331055</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>5</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17d</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17th</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourself</th>\n",
       "      <td>1</td>\n",
       "      <td>13.330078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourselves</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>11</td>\n",
       "      <td>21.328125</td>\n",
       "      <td>15.559989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youthful</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youths</th>\n",
       "      <td>0</td>\n",
       "      <td>6.665039</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yury</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ywinski</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zabul</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.895982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zachary</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.223995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zainab</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.559989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zakhar</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zapatista</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zb</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealand</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealous</th>\n",
       "      <td>0</td>\n",
       "      <td>5.332031</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zedlewski</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>13</td>\n",
       "      <td>10.664062</td>\n",
       "      <td>59.127957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeroed</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeroing</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ziegler</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zillions</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zimring</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zingales</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zionist</th>\n",
       "      <td>0</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zirp</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zittrain</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zones</th>\n",
       "      <td>1</td>\n",
       "      <td>5.332031</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoning</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14793 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lib        con       neut\n",
       "000          64  31.992188  62.239954\n",
       "01            1   0.000000   0.000000\n",
       "03            2   0.000000   0.000000\n",
       "06            4   0.000000   0.000000\n",
       "08            5   0.000000   0.000000\n",
       "09            6   0.000000   0.000000\n",
       "093           0   0.000000   3.111998\n",
       "10           25  26.660156  28.007979\n",
       "100          23  13.330078  43.567968\n",
       "1000          3   0.000000   0.000000\n",
       "109           1   0.000000   0.000000\n",
       "1098          0   1.333008   0.000000\n",
       "10th          0   0.000000  15.559989\n",
       "11           15  10.664062  31.119977\n",
       "12           16  21.328125   3.111998\n",
       "13            5   3.999023   0.000000\n",
       "14            8   5.332031   0.000000\n",
       "143           5   0.000000   0.000000\n",
       "14th          0   2.666016   0.000000\n",
       "15           22  10.664062   0.000000\n",
       "150           7  13.330078   0.000000\n",
       "1500          0   1.333008   0.000000\n",
       "16           23   1.333008   0.000000\n",
       "17            8  10.664062  15.559989\n",
       "175           0   9.331055   0.000000\n",
       "1751          0   0.000000   3.111998\n",
       "1764          0   0.000000   3.111998\n",
       "1776          5   1.333008   0.000000\n",
       "17d           1   0.000000   0.000000\n",
       "17th          4   0.000000   0.000000\n",
       "...         ...        ...        ...\n",
       "yourself      1  13.330078   0.000000\n",
       "yourselves    0   1.333008   0.000000\n",
       "youth        11  21.328125  15.559989\n",
       "youthful      5   0.000000   0.000000\n",
       "youths        0   6.665039   0.000000\n",
       "yury          1   0.000000   0.000000\n",
       "ywinski       1   0.000000   0.000000\n",
       "zabul         0   0.000000  24.895982\n",
       "zachary       0   0.000000   6.223995\n",
       "zainab        0   0.000000  15.559989\n",
       "zakhar        0   0.000000   3.111998\n",
       "zapatista     1   0.000000   0.000000\n",
       "zb            0   1.333008   0.000000\n",
       "zeal          8   0.000000   0.000000\n",
       "zealand       0   1.333008   0.000000\n",
       "zealous       0   5.332031   0.000000\n",
       "zedlewski     1   0.000000   0.000000\n",
       "zero         13  10.664062  59.127957\n",
       "zeroed        3   0.000000   0.000000\n",
       "zeroing       2   0.000000   0.000000\n",
       "ziegler       1   0.000000   0.000000\n",
       "zillions      5   0.000000   0.000000\n",
       "zimring       1   0.000000   0.000000\n",
       "zingales      0   1.333008   0.000000\n",
       "zionist       0   1.333008   0.000000\n",
       "zirp          0   0.000000   3.111998\n",
       "zittrain      1   0.000000   0.000000\n",
       "zone          3   0.000000   0.000000\n",
       "zones         1   5.332031   0.000000\n",
       "zoning        3   0.000000   0.000000\n",
       "\n",
       "[14793 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = pd.DataFrame.from_dict(libdict, orient='index')\n",
    "c = pd.DataFrame.from_dict(condict, orient='index')\n",
    "n = pd.DataFrame.from_dict(neutdict, orient='index')\n",
    "\n",
    "comps = pd.concat([l, c, n], axis=1)\n",
    "comps2 = comps.fillna(0)\n",
    "comps2.columns= (['lib','con','neut'])\n",
    "\n",
    "comps2.loc[:,'con'] *= conmult\n",
    "comps2.loc[:,'neut'] *= neutmult\n",
    "\n",
    "word = []\n",
    "diff = []\n",
    "most = []\n",
    "\n",
    "for i in range(len(comps)):\n",
    "    word.append(comps2.index[i])\n",
    "    rowvals = []\n",
    "    for j in range(len(comps2.columns)):\n",
    "        rowvals.append(comps2.iloc[i,j])\n",
    "    if max(rowvals) == rowvals[0]:\n",
    "        most.append('lib')\n",
    "    elif max(rowvals) == rowvals[1]:\n",
    "        most.append('con')\n",
    "    else:\n",
    "        most.append('neut')\n",
    "    rowvals.sort()\n",
    "    diff.append((rowvals[-1]-rowvals[-2])*((rowvals[-1]-rowvals[-2])/rowvals[-1]))\n",
    "\n",
    "comps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liblist = []\n",
    "liblistb = []\n",
    "conlist = []\n",
    "conlistb = []\n",
    "neutlist = []\n",
    "neutlistb = []\n",
    "\n",
    "for i in range(len(word)):\n",
    "    if most[i] == 'lib':\n",
    "        liblist.append(word[i])\n",
    "        liblistb.append(diff[i])\n",
    "    elif most[i] == 'con':\n",
    "        conlist.append(word[i])\n",
    "        conlistb.append(diff[i])\n",
    "    else:\n",
    "        neutlist.append(word[i])\n",
    "        neutlistb.append(diff[i])\n",
    "\n",
    "libs = np.column_stack((liblist, liblistb))\n",
    "cons = np.column_stack((conlist, conlistb))\n",
    "neuts = np.column_stack((neutlist, neutlistb))\n",
    "\n",
    "dictsize = 3000\n",
    "\n",
    "libcues = []\n",
    "libs2 = libs[libs[:,1].argsort()[::-1]]\n",
    "# print(\"\")\n",
    "# print(\"Liberal:\")\n",
    "for i in range(dictsize):\n",
    "#     print(libs2[i][0])\n",
    "    libcues.append(libs2[i][0])\n",
    "\n",
    "concues = []\n",
    "cons2 = cons[cons[:,1].argsort()[::-1]]\n",
    "# print(\"\")\n",
    "# print(\"Conservative:\")\n",
    "for i in range(dictsize):\n",
    "#     print(cons2[i][0])\n",
    "    concues.append(cons2[i][0])\n",
    "\n",
    "neutcues = []\n",
    "neuts2 = neuts[neuts[:,1].argsort()[::-1]]\n",
    "# print(\"\")\n",
    "# print(\"Neutral:\")\n",
    "for i in range(dictsize):\n",
    "#     print(neuts2[i][0])\n",
    "    neutcues.append(neuts2[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = len(data_all)\n",
    "clip = np.zeros((length, 90))\n",
    "\n",
    "for i in range(length):\n",
    "    s = data_all[i].split()\n",
    "    noncues = 0\n",
    "    position = 0\n",
    "    for z in range(len(s)):\n",
    "        if z == 0:\n",
    "            if s[z] in concues:\n",
    "                clip[i,position] = 100\n",
    "                position += 1\n",
    "                noncues = 0\n",
    "            elif s[z] in libcues:\n",
    "                clip[i,position] = 300\n",
    "                position += 1\n",
    "                noncues = 0\n",
    "            elif s[z] in neutcues:\n",
    "                clip[i,position] = 200\n",
    "                position += 1\n",
    "                noncues = 0\n",
    "            else:\n",
    "                noncues += 1\n",
    "        else:\n",
    "            if clip[i,(position-1)] > 99:\n",
    "                if s[z] in concues:\n",
    "                    clip[i,position] = 100\n",
    "                    position +=1\n",
    "                    noncues = 0\n",
    "                elif s[z] in libcues:\n",
    "                    clip[i,position] = 300\n",
    "                    position +=1\n",
    "                    noncues = 0\n",
    "                elif s[z] in neutcues:\n",
    "                    clip[i,position] = 200\n",
    "                    position +=1\n",
    "                    noncues = 0\n",
    "                else:\n",
    "                    noncues += 1\n",
    "            else:\n",
    "                if s[z] in concues:\n",
    "                    clip[i,position+1] = 100\n",
    "                    clip[i,(position)] = noncues\n",
    "                    position +=2\n",
    "                    noncues = 0\n",
    "                elif s[z] in libcues:\n",
    "                    clip[i,position+1] = 300\n",
    "                    clip[i,(position)] = noncues\n",
    "                    position +=2\n",
    "                    noncues = 0\n",
    "                elif s[z] in neutcues:\n",
    "                    clip[i,position+1] = 200\n",
    "                    clip[i,(position)] = noncues\n",
    "                    position +=2\n",
    "                    noncues = 0\n",
    "                else:\n",
    "                    noncues += 1\n",
    "        if s[len(s)-1] not in concues and libcues and neutcues:\n",
    "            clip[i, position] = noncues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concues: ['abortion', 'themselves', 'allowing', 'explicit', 'pregnancy', 'anxiety', 'planners', 'existing', 'asbestos', 'fascism', 'inherent', 'citizenry', 'constitutes', 'rand', 'trends', 'catholics', 'respiratory', 'hook', 'departments', 'principle']\n",
      "Libcues: ['cut', 'll', 'hedge', 'marginal', 'cities', 'offshore', 'battling', 'share', '22', 'connected', 'congressmen', 'ca', 'banking', 'agriculture', 'working', 'power', 'elections', 'beings', 'controlled', 'pharmaceutical']\n",
      "Neutcues: ['terrified', 'farming', 'bunch', 'dust', 'sciences', 'mediocre', 'predisposition', 'electrical', 'carnage', 'twist', 'analogy', 'towards', 'invoked', 'metals', 'ray', 'book', 'victim', 'cars', 'institution', 'assets']\n"
     ]
    }
   ],
   "source": [
    "print (\"Concues:\", concues[0:20])\n",
    "print (\"Libcues:\", libcues[0:20])\n",
    "print (\"Neutcues:\", neutcues[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.  100.  100. ...,    0.    0.    0.]\n",
      " [ 300.  100.  300. ...,    0.    0.    0.]\n",
      " [   6.  200.  100. ...,    0.    0.    0.]\n",
      " ..., \n",
      " [   2.  100.  100. ...,    0.    0.    0.]\n",
      " [ 100.  100.  100. ...,    0.    0.    0.]\n",
      " [ 100.  300.  300. ...,    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "print (clip[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621, 15)\n",
      "(4525,)\n",
      "(18096,)\n",
      "(4525, 15)\n",
      "(18096, 15)\n",
      "Logistic Regression:\n",
      "C: 0.0001 Score: 0.525303867403\n",
      "C: 0.001 Score: 0.529944751381\n",
      "C: 0.01 Score: 0.562872928177\n",
      "C: 0.1 Score: 0.565524861878\n",
      "C: 1.0 Score: 0.565524861878\n",
      "C: 2.0 Score: 0.565524861878\n",
      "C: 5.0 Score: 0.565524861878\n",
      "C: 50 Score: 0.565524861878\n",
      "C: 100 Score: 0.565303867403\n",
      "Multinomial Naive Bayes:\n",
      "alpha: 0.0001 Accuracy: 0.523314917127\n",
      "alpha: 0.01 Accuracy: 0.523314917127\n",
      "alpha: 0.05 Accuracy: 0.523314917127\n",
      "alpha: 0.1 Accuracy: 0.523314917127\n",
      "alpha: 0.2 Accuracy: 0.523314917127\n",
      "alpha: 1.0 Accuracy: 0.523314917127\n",
      "KNearestNeighbors:\n",
      "K: 1 F1: 0.577320858197\n",
      "K: 3 F1: 0.613335843616\n",
      "K: 5 F1: 0.629597089884\n",
      "K: 7 F1: 0.632716107556\n",
      "K: 9 F1: 0.632822907703\n",
      "K: 11 F1: 0.643381042545\n",
      "K: 13 F1: 0.638697782804\n",
      "K: 15 F1: 0.641797788277\n",
      "K: 1000 F1: 0.595457611824\n",
      "Accuracy (a decision tree): 0.605524861878\n",
      "Accuracy (a random forest): 0.672486187845\n",
      "Accuracy (adaboost with decision trees): 0.667624309392\n"
     ]
    }
   ],
   "source": [
    "#split clip into train & test, labels too\n",
    "clip = np.delete(clip, np.s_[15:], 1)\n",
    "slice2 = int(.8*labs_all.shape[0])\n",
    "clip_train_data = clip[:slice2]\n",
    "clip_train_labels = labs_all[:slice2]\n",
    "clip_test_data = clip[slice2:]\n",
    "clip_test_labels = labs_all[slice2:]\n",
    "print(clip.shape)\n",
    "print(clip_test_labels.shape)\n",
    "print(clip_train_labels.shape)\n",
    "print(clip_test_data.shape)\n",
    "print(clip_train_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "for c in [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0, 5.0, 50, 100]:\n",
    "    logreg = LogisticRegression(C=c, penalty='l1')\n",
    "    logreg.fit(clip_train_data, clip_train_labels)\n",
    "    logregPred = logreg.predict(clip_test_data)\n",
    "    print(\"C:\", c, \"Score:\", logreg.score(clip_test_data, clip_test_labels))\n",
    "        \n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "for a in [0.0001, 0.01, .05, 0.1, 0.2, 1.0]:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(clip_train_data, clip_train_labels)\n",
    "    mnbpreds = mnb.predict(clip_test_data)\n",
    "    print(\"alpha:\", a, \"Accuracy:\", mnb.score(clip_test_data, clip_test_labels))\n",
    "        \n",
    "\n",
    "print(\"KNearestNeighbors:\")\n",
    "for k in [1,3,5,7,9,11,13,15,1000]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(clip_train_data, clip_train_labels)\n",
    "    knnpreds = knn.predict(clip_test_data)\n",
    "    print(\"K:\", k, \"F1:\", metrics.f1_score(clip_test_labels,knnpreds,average='weighted'))\n",
    "    \n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(clip_train_data, clip_train_labels)\n",
    "print ('Accuracy (a decision tree):', dt.score(clip_test_data, clip_test_labels))\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(clip_train_data, clip_train_labels)\n",
    "print ('Accuracy (a random forest):', rfc.score(clip_test_data, clip_test_labels))\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=50, learning_rate=1.0)\n",
    "abc.fit(clip_train_data, clip_train_labels)\n",
    "print ('Accuracy (adaboost with decision trees):', abc.score(clip_test_data, clip_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621, 4)\n",
      "[[ 15.   2.   2.  22.]\n",
      " [  4.   4.   0.   5.]\n",
      " [  6.   9.   6.  18.]\n",
      " [  8.   3.   0.   6.]\n",
      " [  8.   8.   2.  12.]\n",
      " [  8.  10.   0.  21.]\n",
      " [  3.   0.   0.   6.]\n",
      " [  2.   7.   0.   4.]\n",
      " [  3.   0.   1.  11.]\n",
      " [  2.   1.   1.   4.]\n",
      " [  0.   0.   0.   2.]\n",
      " [ 10.   2.   5.  19.]\n",
      " [  9.   0.   0.   5.]\n",
      " [  2.   0.   2.   3.]\n",
      " [  1.   2.   0.   6.]\n",
      " [  6.   6.   1.   7.]\n",
      " [  9.   9.   0.  17.]\n",
      " [  7.   0.   6.  14.]\n",
      " [  5.   0.   0.   5.]\n",
      " [ 15.   7.   0.  15.]]\n"
     ]
    }
   ],
   "source": [
    "# reformat data as an array of counts for libcues, concues, neutcues, noncues per sentence\n",
    "length = len(data_all)\n",
    "clip3 = np.zeros((length, 4))\n",
    "\n",
    "for i in range(length):\n",
    "    s = data_all[i].split()\n",
    "    z = len(s)\n",
    "    noncs = 0\n",
    "    concs = 0\n",
    "    libcs = 0\n",
    "    neutcs = 0\n",
    "    for p in range(z):\n",
    "        if s[p] in concues:\n",
    "            concs += 1\n",
    "        elif s[p] in libcues:\n",
    "            libcs += 1\n",
    "        elif s[p] in neutcues:\n",
    "            neutcs += 1\n",
    "        else:\n",
    "            noncs += 1\n",
    "    clip3[i,0] = concs\n",
    "    clip3[i,1] = libcs\n",
    "    clip3[i,2] = neutcs\n",
    "    clip3[i,3] = noncs\n",
    "    \n",
    "print (clip3.shape)\n",
    "print (clip3[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22621, 4)\n",
      "(4525,)\n",
      "(18096,)\n",
      "(4525, 4)\n",
      "(18096, 4)\n",
      "Logistic Regression:\n",
      "C: 0.0001 F1: 0.633882460509\n",
      "C: 0.001 F1: 0.692464064635\n",
      "C: 0.01 F1: 0.685648960571\n",
      "C: 0.1 F1: 0.683876132833\n",
      "C: 1.0 F1: 0.683820291977\n",
      "C: 2.0 F1: 0.683820291977\n",
      "C: 5.0 F1: 0.683820291977\n",
      "C: 50 F1: 0.683820291977\n",
      "C: 100 F1: 0.683820291977\n",
      "Multinomial Naive Bayes:\n",
      "alpha: 0.0001 F1: 0.581842896732\n",
      "alpha: 0.01 F1: 0.581842896732\n",
      "alpha: 0.05 F1: 0.581842896732\n",
      "alpha: 0.1 F1: 0.581842896732\n",
      "alpha: 0.2 F1: 0.581842896732\n",
      "alpha: 1.0 F1: 0.581842896732\n",
      "KNearestNeighbors:\n",
      "K: 1 F1: 0.598116512876\n",
      "K: 3 F1: 0.640221667446\n",
      "K: 5 F1: 0.658437996031\n",
      "K: 7 F1: 0.669908436599\n",
      "K: 9 F1: 0.671558557884\n",
      "K: 11 F1: 0.678329702886\n",
      "K: 13 F1: 0.681617002413\n",
      "K: 15 F1: 0.679879133158\n",
      "K: 1000 F1: 0.676586884675\n",
      "Accuracy (a decision tree): 0.633591160221\n",
      "Accuracy (a random forest): 0.669834254144\n",
      "Accuracy (adaboost with decision trees): 0.685524861878\n"
     ]
    }
   ],
   "source": [
    "# re-run tests for new array\n",
    "slice3 = int(.8*labs_all.shape[0])\n",
    "clip3_train_data = clip3[:slice3]\n",
    "clip3_train_labels = labs_all[:slice3]\n",
    "clip3_test_data = clip3[slice3:]\n",
    "clip3_test_labels = labs_all[slice3:]\n",
    "print(clip3.shape)\n",
    "print(clip3_test_labels.shape)\n",
    "print(clip3_train_labels.shape)\n",
    "print(clip3_test_data.shape)\n",
    "print(clip3_train_data.shape)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "for c in [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0, 5.0, 50, 100]:\n",
    "    logreg = LogisticRegression(C=c, penalty='l2')\n",
    "    logreg.fit(clip3_train_data, clip3_train_labels)\n",
    "    logregPred = logreg.predict(clip3_test_data)\n",
    "    print(\"C:\", c, \"F1:\", metrics.f1_score(clip3_test_labels,logregPred,average='weighted'))\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "for a in [0.0001, 0.01, .05, 0.1, 0.2, 1.0]:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(clip3_train_data, clip3_train_labels)\n",
    "    mnbpreds = mnb.predict(clip3_test_data)\n",
    "    print(\"alpha:\", a, \"F1:\", metrics.f1_score(clip3_test_labels,mnbpreds,average='weighted'))\n",
    "    \n",
    "\n",
    "print(\"KNearestNeighbors:\")\n",
    "for k in [1,3,5,7,9,11,13,15,1000]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(clip3_train_data, clip3_train_labels)\n",
    "    knnpreds = knn.predict(clip3_test_data)\n",
    "    print(\"K:\", k, \"F1:\", metrics.f1_score(clip3_test_labels,knnpreds,average='weighted'))\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(clip3_train_data, clip3_train_labels)\n",
    "print ('Accuracy (a decision tree):', dt.score(clip3_test_data, clip3_test_labels))\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(clip3_train_data, clip3_train_labels)\n",
    "print ('Accuracy (a random forest):', rfc.score(clip3_test_data, clip3_test_labels))\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=50, learning_rate=1.0)\n",
    "abc.fit(clip3_train_data, clip3_train_labels)\n",
    "print ('Accuracy (adaboost with decision trees):', abc.score(clip3_test_data, clip3_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (924077 tokens)\n",
      "Test set: 11468 sentences (237115 tokens)\n",
      "Loaded 22621 sentences (2.83457e+06 tokens)\n",
      "Training set: 18096 sentences (2262321 tokens)\n",
      "Test set: 4525 sentences (572250 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Loading Brown data to train language model to initialize LSTM parameters\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=10000, shuffle=42)\n",
    "\n",
    "# Loading ideological data\n",
    "vocab_ideo, train_ids_ideo, test_ids_ideo, train_labs_ideo, test_labs_ideo = utils_ideo.process_data(\n",
    "    data_all, labs_all, split=0.8, V=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making smaller datasets for model testing\n",
    "mini_train_ids = train_ids[0:1000]\n",
    "mini_test_ids = test_ids[0:1000]\n",
    "mini_train_ids_ideo = train_ids_ideo[0:100]\n",
    "mini_test_ids_ideo = test_ids_ideo[0:100]\n",
    "mini_train_labs_ideo = train_labs_ideo[0:100]\n",
    "mini_test_labs_ideo = test_labs_ideo[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        # Reshape targets to be one long vector\n",
    "        y = y.reshape([-1,1])\n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                    lm.learning_rate_ : learning_rate,\n",
    "                    lm.initial_h_ : h,\n",
    "                    lm.target_y_ : y}\n",
    "        cost, step, h = session.run([loss, train_op, lm.final_h_], feed_dict = feed_dict)\n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "            \n",
    "    final_cost = total_cost / total_batches\n",
    "    \n",
    "    return final_cost, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch_classification(lm, session, batch_iterator, final_h,\n",
    "              train=False, verbose=False,tick_s=10, \n",
    "              learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    #total_logits = np.empty([0,1,10000])\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        \n",
    "        if i == 0:\n",
    "            h = final_h # final state passed from language model\n",
    "\n",
    "        # Reshape targets to be one long vector\n",
    "        y = y.reshape([-1,1])\n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                    lm.learning_rate_ : learning_rate,\n",
    "                    lm.initial_h_ : h,\n",
    "                    lm.target_y_ : y}\n",
    "        cost, step, h = session.run([loss, train_op, lm.final_h_], feed_dict = feed_dict)\n",
    "        \n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "        #total_logits = np.append(total_logits,logits, axis = 0)\n",
    "        \n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    final_cost = total_cost / total_batches\n",
    "            \n",
    "    return final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost, final_state = run_epoch(lm, session, bi, learning_rate=1.0, train=False, verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))\n",
    "\n",
    "# specify classification labels\n",
    "def score_dataset_ideo(cm, session, ids, labels, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator_ideology(ids, labels, batch_size=50, max_time=100)\n",
    "    cost = run_epoch_classification(cm, session, bi, final_state, learning_rate=1.0, train=False, verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up language model and parameters\n",
    "\n",
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "\n",
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, H=200, softmax_ns=200, num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"language_model\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"language_model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:00\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:00:00\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:00:00\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:00:00\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:00:00\n",
      "Train set: avg. loss: 8.776  (perplexity: 6473.92)\n",
      "[epoch 5] None\n",
      "Test set: avg. loss: 8.858  (perplexity: 7031.14)\n",
      "[epoch 5] None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# GENERAL LANGUAGE MODEL for initializing parameters\n",
    "####\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.batch_generator(mini_train_ids, batch_size, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.        \n",
    "        cost, final_state = run_epoch(lm, session, bi, learning_rate=learning_rate, \n",
    "                         train=True, verbose=False, tick_s=3600)\n",
    "        \n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset(lm, session, mini_train_ids, name=\"Train set\"))\n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset(lm, session, mini_test_ids, name=\"Test set\"))\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:00\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:00:00\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:00:00\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:00:00\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:00:00\n",
      "Train set: avg. loss: 1.603  (perplexity: 4.97)\n",
      "[epoch 5] None\n",
      "Test set: avg. loss: 1.559  (perplexity: 4.75)\n",
      "[epoch 5] None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# CLASSIFICATION MODEL for predicting liberal, neutral, conservative\n",
    "####\n",
    "\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    # Restore variables from language model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "\n",
    "        bi = utils.batch_generator_ideology(mini_train_ids_ideo,mini_train_labs_ideo, batch_size, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.        \n",
    "        cost = run_epoch_classification(lm, session, bi, final_state, learning_rate=learning_rate,\n",
    "                         train=True, verbose=False, tick_s=3600)\n",
    "        \n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset_ideo(lm, session, mini_train_ids_ideo,mini_train_labs_ideo, name=\"Train set\"))\n",
    "    print ((\"[epoch %d]\" % epoch), score_dataset_ideo(lm, session, mini_test_ids_ideo,mini_test_labs_ideo, name=\"Test set\"))\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above LSTM model was left unfinished, upon receiving advice from James and Ian that a CNN would work better. The additional steps to make the above model work would have been: \n",
    "\n",
    "1) Get it to output accuracy as well as perplexity and loss\n",
    "\n",
    "2) Get it to score entire sentences rather than predicting word by word\n",
    "\n",
    "3) Build the pipeline for intake and scoring of news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model with pre-initialized word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def clean_data_and_labels(sentences, labels):\n",
    "    \"\"\"\n",
    "    Takes an array of sentences and their labels.\n",
    "    Splits the data into words and generates labels. Returns clean sentences and labels.\n",
    "    \"\"\"\n",
    "    # Array to list of sentences\n",
    "    x_text = [s.strip() for s in sentences]\n",
    "    # Clean words\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    \n",
    "    # Generate labels\n",
    "    # Map: Liberal --> (1,0,0), Neutral --> (0,1,0), Conservative --> (0,0,1)\n",
    "    y = []\n",
    "    for i in range(0, labels.shape[0]):\n",
    "        if labels[i] == 'Liberal':\n",
    "            y.append([1,0,0])\n",
    "        elif labels[i] == 'Conservative':           \n",
    "            y.append([0,0,1])\n",
    "        else:\n",
    "            y.append([0,1,0])\n",
    "            \n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    words = []\n",
    "    for i in range(0,corpus.shape[0]):\n",
    "        words += corpus[i].split()\n",
    "    token_feed = (canonicalize_word(w) for w in words)\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab = build_vocab(data_all, V = 14836)\n",
    "\n",
    "# Load data\n",
    "x_raw, y = clean_data_and_labels(data_all, labs_all)\n",
    "\n",
    "# Map data into vocabulary\n",
    "max_sentence_len = len(max(data_all, key=len).split())\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_sentence_len)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "\n",
    "# Split up vocabulary\n",
    "split = int(0.9*x.shape[0])\n",
    "x_train, x_dev = x[:split], x[split:]\n",
    "y_train, y_dev = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: The GoogleNews vectors file has not been uploaded to GitHub because of space issues. To make this piece of code work, download google pretrained word vectors from https://code.google.com/archive/p/word2vec/ (file called GoogleNews-vectors-negative300.bin.gz) and change file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load_word2vec_format('/Users/megan/Downloads/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14836, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating embeddings of pre-trained word vectors\n",
    "\n",
    "# Initialize start, stop, and unk words randomly\n",
    "start = np.random.rand(300,)\n",
    "stop = np.random.rand(300,)\n",
    "unk = np.random.rand(300,)\n",
    "embeddings = np.vstack((start, stop, unk))\n",
    "\n",
    "# Loop through words and pull initialized embeddings\n",
    "for i in range(3, len(vocab.ordered_words())):\n",
    "    try:\n",
    "        vector = model.wv[vocab.ordered_words()[i]]\n",
    "    except KeyError: # the word does not have a pre-initialized vector\n",
    "        vector = np.random.rand(300,) #initialize randomly\n",
    "    \n",
    "    embeddings = np.vstack((embeddings,vector))\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Note: CNN code below is adapted from Denny Britz's blog post, which can be found at http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the graph\n",
    "class initialized_CNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, \n",
    "                 vocab_size,embedding_size, filter_sizes, \n",
    "                 num_filters, embedding):\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"Embedding_Layer\"):\n",
    "            W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                trainable=True, name=\"W\")\n",
    "            self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Max-pooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * 3 #len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "        # Define outputs\n",
    "        with tf.name_scope(\"Output_Layer\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"Cost_Function\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        \n",
    "        # Calculate Accuracy to compare to other models\n",
    "        with tf.name_scope(\"Accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/megan/Documents/W266_Classification_Model/runs/1493530668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building the graph\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = initialized_CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=3,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=300,\n",
    "            filter_sizes=map(int, '3,4,5'.split(\",\")),\n",
    "            num_filters=128,\n",
    "            embedding = embeddings\n",
    "        )\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: embeddings})\n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining an epoch\n",
    "def train_epoch(x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training epoch\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 0.5\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    #time_str = datetime.datetime.now().isoformat()\n",
    "    #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "def dev_epoch(x_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy, predictions = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate batches\n",
    "def batch_generator(data, labels, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index], labels[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:38:31.786142: step 50, loss 1.05338, acc 0.393725\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:39:09.640819: step 100, loss 1.00178, acc 0.556783\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:39:49.160958: step 150, loss 0.956692, acc 0.552806\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:40:28.648216: step 200, loss 0.92562, acc 0.564295\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:41:08.198539: step 250, loss 0.902696, acc 0.587715\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:41:47.631844: step 300, loss 0.878081, acc 0.601856\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:42:32.797263: step 350, loss 0.857334, acc 0.628369\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:43:18.266635: step 400, loss 0.835212, acc 0.653999\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:44:04.866790: step 450, loss 0.807814, acc 0.668582\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:44:47.912060: step 500, loss 0.780975, acc 0.680513\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:45:36.497680: step 550, loss 0.753854, acc 0.708352\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:46:24.337434: step 600, loss 0.723325, acc 0.713213\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:47:13.038251: step 650, loss 0.697165, acc 0.730888\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:48:15.770217: step 700, loss 0.677007, acc 0.751215\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:49:18.927484: step 750, loss 0.650313, acc 0.761379\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:50:10.345498: step 800, loss 0.630579, acc 0.770217\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:51:06.536723: step 850, loss 0.611129, acc 0.773752\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:52:04.824680: step 900, loss 0.591601, acc 0.78745\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:53:09.920510: step 950, loss 0.572892, acc 0.795404\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:53:53.452859: step 1000, loss 0.55956, acc 0.790985\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:54:32.192531: step 1050, loss 0.54267, acc 0.801149\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:55:11.340306: step 1100, loss 0.530878, acc 0.804242\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:55:51.682265: step 1150, loss 0.521855, acc 0.811754\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:56:31.439179: step 1200, loss 0.510644, acc 0.815289\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:57:17.634368: step 1250, loss 0.496136, acc 0.823243\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:57:57.249111: step 1300, loss 0.488617, acc 0.819266\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:58:39.697785: step 1350, loss 0.48084, acc 0.819708\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T22:59:20.376231: step 1400, loss 0.471965, acc 0.824569\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T23:00:01.280156: step 1450, loss 0.468157, acc 0.82236\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T23:00:44.094927: step 1500, loss 0.463957, acc 0.823685\n",
      "\n",
      "Saved model checkpoint to /Users/megan/Documents/W266_Classification_Model/runs/1493530668/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2017-04-29T23:01:24.993459: step 1550, loss 0.456747, acc 0.828546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate batches\n",
    "batches = batch_generator(x_train, y_train, batch_size = 64, num_epochs = 5)\n",
    "\n",
    "# Run model with a training loop\n",
    "for batch in batches:\n",
    "    x_batch, y_batch = batch\n",
    "    train_epoch(x_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % 50 == 0: # evaluate every 50 steps\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_epoch(x_dev, y_dev, writer=dev_summary_writer)\n",
    "        print(\"\")\n",
    "    if current_step % 100 == 0: # checkpoint every 100 steps\n",
    "        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "        print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing CNN Sentence Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-29T23:02:37.724135: step 1595, loss 0.450142, acc 0.831639\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions for the dev set\n",
    "predictions = dev_epoch(x_dev, y_dev, writer=dev_summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: \n",
      " Map: Liberal --> 0, Neutral --> 1, Conservative --> 2 \n",
      "\n",
      "INCORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 0\n",
      "the old canard that they are hopeless romantics standing athwart the thrust of progress \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 2\n",
      "Predicted Label: 2\n",
      "the Supreme Court 's makeup , and encouragement from pro-life gain on a number of fronts , \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 2\n",
      "Predicted Label: 2\n",
      "can create new markets , increase farm income , and offer rural America something better than just a safety net : a competitive edge \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "a barrel \n",
      "\n",
      "INCORRECT:\n",
      "Correct Label: 0\n",
      "Predicted Label: 2\n",
      "get the idea that government could actually he useful \n",
      "\n",
      "INCORRECT:\n",
      "Correct Label: 0\n",
      "Predicted Label: 1\n",
      "energy use accordingly-shutting off appliances during periods of peak demand , or programming their plug-in hybrids to charge when electricity is cheap \n",
      "\n",
      "INCORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 0\n",
      "The current situation for the Republican Party \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "his truck \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 0\n",
      "Predicted Label: 0\n",
      "sought to impose extremely unpopular oil legislation that would favor foreign oil corporations \n",
      "\n",
      "CORRECT:\n",
      "Correct Label: 1\n",
      "Predicted Label: 1\n",
      "does not jeopardize their states ' investments \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing out examples of predictions in the dev set\n",
    "data_dev = data_all[split:]\n",
    "\n",
    "print(\"Note:\",'\\n',\"Map: Liberal --> 0, Neutral --> 1, Conservative --> 2\",'\\n')\n",
    "for i in range(0, 10):\n",
    "    correct_label = y_dev[i].index(max(y_dev[i]))\n",
    "    if predictions[i] == correct_label:\n",
    "        print(\"CORRECT:\")\n",
    "        print(\"Correct Label:\", correct_label)\n",
    "        print (\"Predicted Label:\", predictions[i])\n",
    "        print(data_dev[i], '\\n')\n",
    "    else:\n",
    "        print(\"INCORRECT:\")\n",
    "        print(\"Correct Label:\", correct_label)\n",
    "        print (\"Predicted Label:\", predictions[i])\n",
    "        print(data_dev[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map: Liberal --> 0, Neutral --> 1, Conservative --> 2\n",
    "# Mislabels error analysis\n",
    "correct_labels = defaultdict(int)\n",
    "con_as_neut = 0\n",
    "con_as_lib = 0\n",
    "lib_as_neut = 0\n",
    "lib_as_con = 0\n",
    "neut_as_con = 0\n",
    "neut_as_lib = 0\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "    correct_label = y_dev[i].index(max(y_dev[i]))\n",
    "    if predictions[i] == correct_label:\n",
    "        correct_labels[labs_test[i]] += 1\n",
    "    else:\n",
    "        if correct_label == 2 and predictions[i] == 1:\n",
    "            con_as_neut += 1\n",
    "        elif correct_label == 2 and predictions[i] == 0:\n",
    "            con_as_lib += 1\n",
    "        elif correct_label == 1 and predictions[i] == 0:\n",
    "            neut_as_lib += 1\n",
    "        elif correct_label == 1 and predictions[i] == 2:\n",
    "            neut_as_con += 1\n",
    "        elif correct_label == 0 and predictions[i] == 1:\n",
    "            lib_as_neut += 1\n",
    "        elif correct_label == 0 and predictions[i] == 2:\n",
    "            lib_as_con += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conservative phrases labeled as neutral: 53\n",
      "Number of conservative phrases labeled as liberal: 90\n",
      "Number of liberal phrases labeled as neutral: 76\n",
      "Number of liberal phrases labeled as conservative: 71\n",
      "Number of neutral phrases labeled as conservative: 37\n",
      "Number of neutral phrases labeled as liberal: 54\n"
     ]
    }
   ],
   "source": [
    "print('Number of conservative phrases labeled as neutral:',con_as_neut)\n",
    "print('Number of conservative phrases labeled as liberal:',con_as_lib)\n",
    "print('Number of liberal phrases labeled as neutral:',lib_as_neut)\n",
    "print('Number of liberal phrases labeled as conservative:',lib_as_con)\n",
    "print('Number of neutral phrases labeled as conservative:',neut_as_con)\n",
    "print('Number of neutral phrases labeled as liberal:',neut_as_lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = dict()\n",
    "\n",
    "# Define text-cleaning functions\n",
    "\n",
    "def replace_punct(input):\n",
    "    input = input.replace(',', '')\n",
    "    input = input.replace('\\'', '')\n",
    "    input = input.replace(':', '')\n",
    "    input = input.replace('[', '')\n",
    "    input = input.replace(']', '')\n",
    "    input = input.replace('(', '')\n",
    "    input = input.replace(')', '')\n",
    "    input = input.replace('_', '')\n",
    "    input = input.replace('\"', '')\n",
    "\n",
    "    input = input.replace('-', ' ')\n",
    "    input = input.replace('+', ' ')\n",
    "\n",
    "    return input\n",
    "\n",
    "def clean_article(text):\n",
    "\n",
    "    # replace certain strings\n",
    "    text = text.replace('expand / contract','')  \n",
    "\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\\\u[0-9]+[abcde]*', '',text) \n",
    "\n",
    "    text = replace_punct(text)\n",
    "    text = re.sub(r'\\d+', '{DG}', text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For Fox News\n",
    "\n",
    "def fox_scraper(url):\n",
    "    r = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    tags = soup.find_all(\"div\", class_=\"article-text\")\n",
    "    \n",
    "    # Clean text\n",
    "    article_text = re.sub(\"<[^>]*>\", \"\", str(tags), flags=re.MULTILINE)\n",
    "    article_text = article_text.replace('\\xa0',' ')\n",
    "    article_text = clean_article(article_text)\n",
    "    \n",
    "    return article_text\n",
    "\n",
    "urls = [\n",
    "    'http://www.foxnews.com/politics/2017/04/18/trump-signs-order-to-clamp-down-on-visa-program-enforce-buy-american-policy.html',\n",
    "    'http://www.foxnews.com/us/2017/04/18/facebook-killer-steve-stephens-dead-police-say.html',\n",
    "    'http://www.foxnews.com/opinion/2017/04/17/care-ceo-famine-is-stalking-globe-and-america-urgently-needs-to-lead.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/18/georgia-race-gop-hits-ossoff-for-living-outside-district-in-final-stretch.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/18/trump-illegal-immigrant-criminals-are-getting-hell-out.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/19/uss-carl-vinson-carrier-was-not-on-way-to-north-korea-reports-say.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/19/iran-nuclear-deal-trump-administration-says-tehran-complying-with-agreement.html',\n",
    "    'http://www.foxnews.com/us/2017/04/19/death-ny-state-judge-found-in-hudson-river-reportedly-suspicious.html',\n",
    "    'http://www.foxnews.com/us/2017/04/19/inspector-who-tried-to-rein-in-illegal-logging-was-fired.html',\n",
    "    'http://www.foxbusiness.com/politics/2017/04/18/with-democratic-invasion-white-house-cuban-starting-to-warm-up-to-trump-presidency.html',\n",
    "    'http://www.foxnews.com/us/2017/04/28/coal-miners-health-care-bailouts-riddled-with-dubious-expenses-audit-says.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/trump-proclaims-may-1-as-loyalty-day.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/trump-at-nra-convention-eight-year-assault-on-gun-rights-is-over.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/gov-jerry-brown-keeps-oroville-dam-repair-costs-hidden-state-lawmakers-say.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/tillerson-presses-for-economic-sanctions-on-north-korea-in-special-un-meeting.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/house-bill-to-give-venezuelans-path-to-legal-us-residency-has-bipartisan-support.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/tillerson-eyes-cutting-2300-jobs-at-state-department.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/treaty-restrictions-giving-china-huge-missile-advantage-over-us-admiral-warns.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/28/new-cold-war-russia-touts-arctic-military-base-as-us-struggles-to-catch-up.html',\n",
    "    'http://www.foxnews.com/politics/2017/04/29/hill-republicans-appear-closer-to-passing-obamacare-overhaul-but-not-close-enough.html'\n",
    "]\n",
    "\n",
    "n = 0\n",
    "for url in urls:\n",
    "    articles['fox_%02d' % n] = fox_scraper(url)\n",
    "    n += 1 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For NYT\n",
    "\n",
    "def nyt_scraper(url):\n",
    "    response = requests.get(url, timeout=1.000)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')   \n",
    "\n",
    "    tag = soup.find_all('p', class_=\"story-body-text story-content\", text=True)\n",
    "    article_text = ''\n",
    "    for t in tag:\n",
    "        article_text += str(t)     \n",
    "    \n",
    "    # Cleaning text\n",
    "    article_text = re.sub(\"<[^>]*>\", \"\", str(article_text), flags=re.MULTILINE)\n",
    "    article_text = clean_article(article_text)\n",
    "    \n",
    "    return article_text\n",
    "\n",
    "urls = [\n",
    "    'https://www.nytimes.com/2017/04/18/world/asia/aircraft-carrier-north-korea-carl-vinson.html?ref=todayspaper&_r=0',\n",
    "    'https://www.nytimes.com/2017/04/18/world/europe/uk-theresa-may-general-election.html?ribbon-ad-idx=3&rref=todayspaper&module=Ribbon&version=origin&region=Header&action=click&contentCollection=Today%E2%80%99s%20Paper&pgtype=article',\n",
    "    'https://www.nytimes.com/2017/04/18/world/europe/hungary-orban-populism-migrants-border-european-union.html?ribbon-ad-idx=3&rref=todayspaper&module=Ribbon&version=origin&region=Header&action=click&contentCollection=Today%E2%80%99s%20Paper&pgtype=article',\n",
    "    'https://www.nytimes.com/2017/04/18/opinion/mr-trump-plays-by-his-own-rules-or-no-rules.html?action=click&pgtype=Homepage&clickSource=story-heading&module=opinion-c-col-left-region&region=opinion-c-col-left-region&WT.nav=opinion-c-col-left-region',\n",
    "    'https://www.nytimes.com/2017/04/18/opinion/defending-governor-cuomos-free-tuition-plan.html?ribbon-ad-idx=16&rref=opinion&module=Ribbon&version=context&region=Header&action=click&contentCollection=Opinion&pgtype=article',\n",
    "    'https://www.nytimes.com/2017/04/18/opinion/my-daughter-is-not-transgender-shes-a-tomboy.html?action=click&pgtype=Homepage&clickSource=story-heading&module=opinion-c-col-right-region&region=opinion-c-col-right-region&WT.nav=opinion-c-col-right-region',\n",
    "    'https://www.nytimes.com/2017/04/19/us/politics/georgia-special-election.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news',\n",
    "    'https://www.nytimes.com/2017/04/18/us/politics/georgia-special-election-jon-ossoff.html?ribbon-ad-idx=6&rref=homepage&module=Ribbon&version=origin&region=Header&action=click&contentCollection=Home%20Page&pgtype=article',\n",
    "    'https://www.nytimes.com/2017/04/18/business/media/fox-bill-oreilly.html?ribbon-ad-idx=6&rref=homepage&module=Ribbon&version=origin&region=Header&action=click&contentCollection=Home%20Page&pgtype=article',\n",
    "    'https://www.nytimes.com/2017/04/18/us/politics/executive-order-hire-buy-american-h1b-visa-trump.html?ribbon-ad-idx=6&rref=homepage&module=Ribbon&version=origin&region=Header&action=click&contentCollection=Home%20Page&pgtype=article',\n",
    "    'https://www.nytimes.com/2017/04/18/us/politics/trump-inauguration-fundraising.html?ribbon-ad-idx=6&rref=homepage&module=Ribbon&version=origin&region=Header&action=click&contentCollection=Home%20Page&pgtype=article&mtrref=www.nytimes.com&gwh=65A993E2EE2CFFC71965945E1349AC8C&gwt=pay',\n",
    "    'https://www.nytimes.com/2017/04/28/us/politics/school-choice-betsy-devos.html?ref=politics&mtrref=www.nytimes.com',\n",
    "    'https://www.nytimes.com/2017/04/28/us/politics/christopher-murphy-guns-trump.html?ref=politics&mtrref=www.nytimes.com',\n",
    "    'https://www.nytimes.com/2017/04/29/us/politics/peoples-climate-march-trump.html?ref=politics',\n",
    "    'https://www.nytimes.com/2017/03/04/world/asia/north-korea-missile-program-sabotage.html?ref=politics',\n",
    "    'https://www.nytimes.com/2017/04/29/us/politics/trump-presidency-100-days.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=a-lede-package-region&region=top-news&WT.nav=top-news',\n",
    "    'https://www.nytimes.com/2017/04/29/world/europe/marine-le-pen-nicolas-dupont-aignan.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news',\n",
    "    'https://www.nytimes.com/2017/04/29/us/united-states-citizenship-and-immigration-services-military-screening.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news',\n",
    "    'https://www.nytimes.com/2017/04/29/us/politics/peoples-climate-march-trump.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news',\n",
    "    'https://www.nytimes.com/2017/04/29/world/middleeast/pope-francis-egypt.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news&_r=0'\n",
    "]\n",
    "\n",
    "\n",
    "n = 0\n",
    "for url in urls:\n",
    "    articles['nyt_%02d' % n] = nyt_scraper(url)\n",
    "    n += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Washington Post\n",
    "\n",
    "def post_scraper(url):\n",
    "    r = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "    article_text = \"\"                    \n",
    "    tag = soup.find_all('div', class_=\"article-body\")\n",
    "\n",
    "    for t in tag:\n",
    "        mytext = str(t)                      \n",
    "        soup2 = BeautifulSoup(mytext, 'html.parser')  \n",
    "        tag2 = soup2.find_all('p')\n",
    "\n",
    "        for t2 in tag2:\n",
    "            article_text += str(t2) \n",
    "\n",
    "    # Cleaning text\n",
    "    article_text = re.sub(\"<[^>]*>\", \"\", str(tag2), flags=re.MULTILINE)\n",
    "    article_text = clean_article(article_text)\n",
    "    article_text = article_text.replace('\\xa0',' ')\n",
    "\n",
    "    return article_text\n",
    "\n",
    "urls = [\n",
    "    'https://www.washingtonpost.com/world/national-security/on-russia-trump-and-his-top-national-security-aides-seem-to-be-at-odds/2017/04/18/13fdc832-23bf-11e7-bb9d-8cd6118e1409_story.html?hpid=hp_hp-top-table-main_trumprussia-740pm%3Ahomepage%2Fstory&utm_term=.57d1a4076ea0',\n",
    "    'https://www.washingtonpost.com/world/national-security/on-north-korea-trump-administration-talks-tough-but-hopes-to-avoid-war/2017/04/18/96d15536-244a-11e7-bb9d-8cd6118e1409_story.html?hpid=hp_hp-top-table-main_usnorthkorea-920pm%3Ahomepage%2Fstory&utm_term=.5647e44a59dd',\n",
    "    'https://www.washingtonpost.com/powerpost/georgia-house-race-stokes-gop-identity-crisis--and-opportunity-for-democrats/2017/04/18/a2231a48-242f-11e7-b503-9d616bd5a305_story.html?hpid=hp_hp-top-table-main_georgia-9am%3Ahomepage%2Fstory&utm_term=.fe02318f77b6',\n",
    "    'https://www.washingtonpost.com/politics/trump-we-may-terminate-us-south-korea-trade-agreement/2017/04/27/75ad1218-2bad-11e7-a616-d7c8a68c1a66_story.html?utm_term=.beb46659ffd1',\n",
    "    'https://www.washingtonpost.com/powerpost/in-its-first-100-days-in-power-the-gop-scrambles-to-learn-how-to-govern/2017/04/28/8a33ffaa-2a84-11e7-a616-d7c8a68c1a66_story.html?utm_term=.bbd551dd36d6',\n",
    "    'https://www.washingtonpost.com/news/post-politics/wp/2017/04/29/protesters-to-hold-opposition-march-as-president-trump-hosts-100-day-rally/?hpid=hp_hp-top-table-main_pp-protest-128pm%3Ahomepage%2Fstory&utm_term=.7836bc80eb7c',\n",
    "    'https://www.washingtonpost.com/national/health-science/climate-march-expected-to-draw-massive-crowd-to-dc-in-sweltering-heat/2017/04/28/1bdf5e66-2c3a-11e7-b605-33413c691853_story.html?hpid=hp_hp-top-table-main_climatemarch-843am%3Ahomepage%2Fstory&utm_term=.12a624d7431b',\n",
    "    'https://www.washingtonpost.com/politics/after-a-tumultous-start-trump-hopes-for-a-smoother-agenda-on-jobs-and-taxes/2017/04/29/2df4c05e-2c45-11e7-b605-33413c691853_story.html?utm_term=.09f20914249a',\n",
    "    'https://www.washingtonpost.com/powerpost/democrats-argue-way-back-to-power-is-through-small-towns-won-by-trump/2017/04/29/d4b50c4a-2c50-11e7-be51-b3fc6ff7faee_story.html?utm_term=.b82d2172435c',\n",
    "    'https://www.washingtonpost.com/local/social-issues/immigrant-rights-marches-work-stoppages-planned-monday/2017/04/28/6150b806-2af4-11e7-b605-33413c691853_story.html?utm_term=.5790f96ac4ea',\n",
    "    'https://www.washingtonpost.com/news/powerpost/wp/2017/04/28/lawmakers-questionable-stock-trades-prompt-new-bill/?utm_term=.828c998c0380',\n",
    "    'https://www.washingtonpost.com/world/other-countries-are-still-trying-to-figure-out-what-trump-means-to-them/2017/04/28/01dbaa98-25fe-11e7-b503-9d616bd5a305_story.html?utm_term=.db20fc5e3030',\n",
    "    'https://www.washingtonpost.com/powerpost/lawmakers-poised-to-approve-one-week-spending-bill-friday-to-keep-government-open/2017/04/28/0bba76da-2c01-11e7-b605-33413c691853_story.html?utm_term=.37fdae66ede3',\n",
    "    'https://www.washingtonpost.com/politics/republicans-are-tethered-to-trump-politically-and-need-to-act-accordingly/2017/04/29/ede6bb38-2cef-11e7-b605-33413c691853_story.html?utm_term=.98263c6aec9f',\n",
    "    'https://www.washingtonpost.com/world/other-countries-are-still-trying-to-figure-out-what-trump-means-to-them/2017/04/28/01dbaa98-25fe-11e7-b503-9d616bd5a305_story.html?utm_term=.d4c3a625727b',\n",
    "    'https://www.washingtonpost.com/local/social-issues/immigrant-rights-marches-work-stoppages-planned-monday/2017/04/28/6150b806-2af4-11e7-b605-33413c691853_story.html?utm_term=.02abf192f10d',\n",
    "    'https://www.washingtonpost.com/news/powerpost/wp/2017/04/28/lawmakers-questionable-stock-trades-prompt-new-bill/?utm_term=.00a4ab9abb26',\n",
    "    'https://www.washingtonpost.com/powerpost/just-knowing-hes-here-makes-me-feel-good-women-at-nra-convention-cheer-trump/2017/04/29/0bc56b5c-2c5c-11e7-b605-33413c691853_story.html?utm_term=.557a28e41f2a',\n",
    "    'https://www.washingtonpost.com/powerpost/house-republicans-introduce-one-week-spending-bill-to-continue-budget-talks/2017/04/27/5157abee-2b3a-11e7-b605-33413c691853_story.html?utm_term=.497d19f3058a',\n",
    "    'https://www.washingtonpost.com/politics/trump-reversing-obama-will-push-to-expand-drilling-in-the-arctic-and-atlantic/2017/04/27/757fa06c-2aae-11e7-b605-33413c691853_story.html?utm_term=.e90e21f4f3cd'\n",
    "    ]\n",
    "\n",
    "n = 0\n",
    "for url in urls:\n",
    "    articles['twp_%02d' % n] = post_scraper(url)\n",
    "    n += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Associated Press\n",
    "\n",
    "def apr_scraper(url):\n",
    "    r = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    tags = soup.find_all(\"p\", class_=\"ap-story-p\")\n",
    "\n",
    "    # Cleaning text\n",
    "    article_text = re.sub(\"<[^>]*>\", \"\", str(tags), flags=re.MULTILINE)\n",
    "    article_text = article_text.replace('&amp;apos;s', '')\n",
    "    article_text = clean_article(article_text)\n",
    "    \n",
    "    return article_text\n",
    "\n",
    "urls = [\n",
    "    'http://hosted.ap.org/dynamic/stories/E/EU_FRANCE_ELECTION?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-16-18-26',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_TRUMP_100_THE_PRESIDENCY?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-12-04-39',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_TRUMP_CONGRESS?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-12-46-14',\n",
    "    'http://hosted.ap.org/dynamic/stories/A/AS_NKOREA_DETAINED_AMERICAN?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-10-22-53',\n",
    "    'http://hosted.ap.org/dynamic/stories/M/ML_ISRAEL_50_YEARS_LATER_PALESTINIAN_REFUGEES?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-02-33-58',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_OBIT_ERIN_MORAN_INOL-?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-08-09-59',\n",
    "    'http://hosted.ap.org/dynamic/stories/S/STRUGGLING_VENEZUELANS?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-12-16-21',\n",
    "    'http://hosted.ap.org/dynamic/stories/A/AF_KENYA_NOVELIST_SHOT?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-13-45-09',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_FEMALE_GENITAL_MUTILATION_MIOL-?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-10-54-21',\n",
    "    'http://hosted.ap.org/dynamic/stories/E/EU_FRANCE_ELECTION_WHY_IT_MATTERS?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-23-12-45-03',\n",
    "    'http://hosted.ap.org/dynamic/stories/E/EU_TURKEY_SYRIA_KURDS?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-16-27-07',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_ECONOMY_GDP?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-01-47-40',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_CONGRESS_RDP?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-28-20-25-29',\n",
    "    'http://hosted.ap.org/dynamic/stories/K/KOREAS_TENSION_ASOL-?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-20-13-37',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_RODNEY_KING_RIOTS_25TH_ANNIVERSARY?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-17-24-53',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_FUTURE_OF_EXECUTIONS_AROL-?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-14-22-52',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_METH_MENTALLY_ILL_INMATES?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-15-45-20',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_TRUMP?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-21-13-33',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_TRUMP_100_CLIMATE_MARCHES?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-19-21-24',\n",
    "    'http://hosted.ap.org/dynamic/stories/U/US_MONTANA_SPECIAL_ELECTION_DEBATE?SITE=AP&SECTION=HOME&TEMPLATE=DEFAULT&CTIME=2017-04-29-22-32-13'\n",
    "]\n",
    "\n",
    "\n",
    "n = 0\n",
    "for url in urls:\n",
    "    articles['apr_%02d' % n] = apr_scraper(url)\n",
    "    n += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox_00 :  president trump on tuesday signed an executive order that will make it harder for american tech companies to hire employees from foreign countries willing to work for less money than americans. trump signed the buy american hire american order during his visit to the snap on tools headquarters in kenosha wis. the president called the order bold new steps toward making good on his campaign promises to generate more jobs for out of work americans. trump called the order a powerful signal to the world that finally puts america first. we are finally standing up for our workers and our companies trump said. the order targets the h{DG} b visa program which allows u.s. companies to employ graduate level workers in specialty occupations like it engineering mathematics and science. among other changes the white house wants to end the h{DG} b lottery system and replace it with a merit based one   though its unclear exactly what criteria they would use.   were going to switch away from a random lottery system in which its weighted toward the lowest wage workers towards a system that prioritizes higher skilled higher paid workers which would make it much more difficult to use it to replace american workers a senior administration official said monday.   each year on april {DG} a fresh cap for h{DG} b visa applications is set by u.s. citizenship and immigration services. under the current system applications are then randomly selected in a lottery system. trumps order also empowers federal agencies to reexamine loopholes in the governments procurement process. specifically they would look into whether waivers in free trade agreements are leading to unfair trade by companies outside the u.s. and whether it undercuts american companies on a global playing field. on the campaign trail trump repeatedly vilified companies that looked to hire foreign workers. he vowed to end the h{DG} b program which he said allowed big business to fire americans and replace them with foreigners. currently the governments h {DG}b visa program admits {DG} immigrants in each year to handle high tech jobs. the number of application for h{DG} b visas fell to {DG} this year from {DG} in {DG} and {DG} in {DG} according to u.s. citizenship and immigration services. trumps order also requires applicants and their employers to demonstrate that the hb {DG} visas awards will only go to the most highly skilled workers in their fields. trump has come under fire for not practicing what he preaches. while he has pledged to support american goods and workers some of his trump branded products are made overseas or made by foreign workers. the president  also has been accused of looking the other way when his son eric trump asked to bring in {DG} workers to work at trump vineyard estates through the federal h {DG}a visa program. that program enables agricultural employers to bring in seasonal foreign workers. according to filings job orders for trump vineyard estates say the primary tasks include planting and cultivating vines adding grow tubes and pruning grape vines. during his wide ranging speech trump also promised to fix the nations crumbling infrastructure beef up trade deals tweak the tax code and pass a new health care bill that will replace and repeal obamacare. he also said he wanted to work with lawmakers on getting wisconsin dairy workers to get into the canadian market.      \n",
      "\n",
      "nyt_00 : the problem was that the carrier the carl vinson and the three other warships in its strike force were that very moment sailing in the opposite direction to take part in joint exercises with the australian navy in the indian ocean {DG} miles southwest of the korean peninsula.white house officials said tuesday that they had been relying on guidance from the defense department. officials there described a glitch ridden sequence of events from an ill timed announcement of the deployment by the militarys pacific command to a partially erroneous explanation by the defense secretary jim mattis  all of which perpetuated the false narrative that a flotilla was racing toward the waters off north korea.with mr. trump himself playing up the show of force pentagon officials said rolling back the story became difficult.now the carl vinson is finally on a course for the korean peninsula expected to arrive in the region next week according to defense department officials. white house officials declined to comment on the confusion referring questions to the pentagon. sean discussed it once when asked and it was all about process a spokesman michael short said of mr. spicer.privately however other officials expressed bewilderment that the pentagon did not correct its timeline particularly given the tensions in the region and the fact that mr. spicer as well as the national security adviser lt. gen. h. r. mcmaster were publicly answering questions about it.the ship is now moving north to the western pacific the pentagons chief spokeswoman dana white said tuesday. this should have been communicated more clearly at the time.given the timing it hardly needed to mr. trump had just wrapped up a two day summit meeting with mr. xi at his palm beach club mar a lago with a message that the united states had run out of patience with north koreas dictator kim jong un and its nuclear and missile programs.that sunday general mcmaster told fox news that the deployment was a prudent move designed to give the president a full range of options to remove the threat posed by mr. kim.what the navy did not say was that the carl vinson had to carry out another mission before it set sail north a long scheduled joint exercise with the australian navy in the indian ocean.i dont call audibles with aircraft carriers he said using a football metaphor to reject the midcourse correction.by all accounts mr. trump is less worried than mr. obama about making such calls on the fly. his aides have praised this unpredictability as a virtue in dealing with rogue leaders in north korea and syria.in south korea though fears of a full blown war erupted. the government rushed to reassure the public that the carl vinson was coming only to deter north korean provocations. april {DG} is the birthday of kim il sung the nations founder and the grandfather of kim jong un  an occasion the north typically uses to conduct celebratory weapons tests.a carrier group is several things mr. spicer replied. the forward deployment is deterrence presence. he added i think when you see a carrier group steaming into an area like that the forward presence of that is clearly through almost every instance a huge deterrence.mr. spicer did not point out that the carl vinson was not in fact steaming into the area and would not be for {DG} more days. a senior administration official said the press secretary was using talking points supplied by the pentagon. he was discussing the rationale for sending a carrier this official said not confirming the ships schedule.an hour after mr. spicer left the podium mr. mattis the defense secretary reinforced the perception of ships racing to the scene. speaking at the pentagon he said the navy disclosed the carl vinsons itinerary in advance because the exercise with the australians had been canceled. we had to explain why she wasnt in that exercise he said.mr. mattis however had conflated two things admiral harris had canceled only a port call for the carl vinson in fremantle australia according to pentagon officials because he feared that images of sailors on shore leave would be unseemly at a time when north korea was firing missiles.navy officials said admiral harris never meant to suggest he was canceling the naval exercise. organizing such exercises is a complicated effort that takes months. one official described it as a high end exercise raising the possibility that the two navies practiced scenarios to counter china or tested new missile defenses or cyberoperations.some officials expressed irritation with admiral harris saying he did not think through the consequences of announcing the deployment of an aircraft carrier during a period of high tension.mr. mattis sent mixed signals about the mission. he stressed the need for the navy to operate freely in the pacific but added theres not a specific demand signal or specific reason why were sending her up there.after a week of war drums fueled by the reports of the oncoming armada tensions subsided when the weekend passed with only a military parade in pyongyang and a failed missile test. \n",
      "\n",
      "twp_00 : the message was defiantly optimistic like a suitor determined to hold a relationship together despite mounting obstacles. things will work out fine between the u.s.a. and russia president trump declared on his twitter account last week. at the right time everyone will come to their senses &amp; there will be lasting peace! trumps interest in achieving warm relations with moscow has been a consistent theme since the earliest days of his campaign and it stands now as one of the few major foreign policy positions that he has not discarded or revised since taking office. but in his devotion to this outcome trump appears increasingly isolated within his own administration. over the past several weeks senior members of trumps national security team have issued blistering critiques of moscow using harsh terms that have led to escalating tensions between the countries and that seem at odds with the president. the harsh rhetoric  and the apparent lack of any rebuke from trump  suggests that russian skeptics have gained influence in the administration making the rapprochement that trump envisioned seem increasingly remote. in a speech at the united nations u.s. ambassador nikki haley lashed out at russia for its role in syria asking how many more children have to die before russia cares enough to prevent syrian president bashar al assad from committing further atrocities. secretary of state rex tillerson accused russia of being incompetent or complicit in the chemical weapons attack that killed dozens of syrian civilians. cia director mike pompeo went even further in an appearance at the center for strategic and international studies in washington last week depicting moscow as an unredeemable adversary. though trump has repeatedly praised russian president vladimir putin pompeo described him as a man for whom veracity doesnt translate into english. the statements have created confusion about the trump administrations posture toward russia and put senior officials including haley in the awkward position of having to explain why trump has yet to echo any of their strong words. the white house did not respond to requests for comment. foreign policy experts close to the administration played down the apparent disconnect between trumps statements and those of his national security subordinates saying that trumps words about russia were often misinterpreted to signal that he intended to be soft. there was never anything in the plan about being nice to the russians said james carafano the vice president of foreign and defense policy at the heritage foundation who served as an adviser to trump during the campaign and post election transition. i dont think any of this is a u turn a reversal or a shift carafano said. he noted that trumps decision to bomb an airstrip in syria where the russian military had worked with assads forces and trumps recent vocal support for nato demonstrate his willingness to defy putin.  trump doesnt have to do russia bashing and is probably seeking to leave an opening for putin to pursue better relations with the united states carafano said. the fact that trumps officials are not mimicking the exact same words doesnt mean theyre not on the same sheet of music. in recent weeks trump has had opportunities to reinforce the messages of his subordinates. in a news conference with nato secretary general jens stoltenberg this month trump said relations with moscow may be at an all time low but described russia as a strong country and said were going to see how that all works out. asked about growing concerns in europe over alleged moscow interference in elections and calls for bolstering europes military defenses trump had no words of caution for the kremlin. right now there is a fear and there are problems trump said. but ultimately i hope that there wont be a fear and there wont be problems and the world can get along. that would be the ideal situation. trumps tack with russia seems at odds with his approach toward other global powers and issues. he threatened to label china a currency manipulator and to cut off u.s. support for nato for example before retreating from those positions in recent weeks. trumps posture toward moscow is also seen as a reflection of his reluctance to acknowledge that russia interfered in the u.s. election and based on the consensus view of u.s. intelligence agencies sought to help him win. critics said the administrations competing messages have caused concern overseas. rep. adam b. schiff calif. the ranking democrat on the house intelligence committee said that he recently attended a security conference in munich where there were profound questions among our allies about just where this administration is coming from. they dont see the president yet willing to take on putin or to criticize him directly schiff said. it doesnt matter what others in his cabinet said. if they didnt hear it from the president they didnt really believe it was administration policy. senior administration officials have struggled to explain the disparity in their comments  including statements suggesting that russia may have known that assad was about to launch a chemical weapons attack  with those of the president. i think were both saying the same thing; its just being reported differently haley said during an interview on abc news this month. pressed on why trump has not condemned moscow haley said this is what i can tell you the president has not once called me and said dont beat up on russia has not once called me and told me what to say. trumps national security adviser h.r. mcmaster faced similar questions in a separate abc interview this week when asked how the president could be so confident that things will work out fine and predict lasting peace. well mcmaster quipped when relations are at the lowest point theres nowhere to go but up. mcmaster has helped form the administrations more combative stance toward moscow. he replaced michael flynn who seemed to share trumps interest in pursuing closer relations with moscow before flynn was fired for his misleading statements about his contacts with the russian ambassador. juan zarate a former national security official who advised pompeo during his confirmation as cia chief said that he sees trumps continued conciliatory messages toward moscow as a means of preserving options for the administration in its dealings with russia. i worry less about what appears to be some discordance because i think you can have flexibility in messaging zarate said. but you do have to have consistency in policy. for now it seems like we do. in fact the policy seems to be getting more vigorous and confrontational. but zarate also noted trumps tendency to double down on positions. trump was criticized for seeming lenient toward moscow and lo and behold hes going to stick to his line. moscow has also noticed the administrations competing messages. after a series of sharp exchanges with senior u.s. officials russian foreign minister sergei lavrov said this week that moscow would focus on signals from the president. we will be guided by what president donald trump once again confirmed ... that he wants to improve relations with the russian federation lavrov said. we are also ready for that. \n",
      "\n",
      "apr_00 :           usseau france        ap    french presidential front runner emmanuel macron hunted saturday for votes in rural france where his far right opponent marine le pen is making inroads among country folk who feel left behind. back in paris le pen announced that if she wins the presidency in the may {DG} runoff she would name former rival nicolas dupont aignan her new campaign ally as her prime minister. the move aims to secure the nearly {DG}.{DG} million votes that the anti european union conservative got when he was eliminated from the presidential race in the first round of balloting. since many dupont aignan voters had already been expected to switch to le pen for her runoff against the centrist macron the alliance is unlikely to prove a massive electoral boost for her. symbolically however it punctured a hole in hopes   expressed by mainstream politicians on both the left and the right   that french voters would unite against le pen extremism in the runoff. that did happen in {DG} when her father jean marie le pen made it to the presidential runoff but lost overwhelmingly to jacques chirac. at a news conference with dupont aignan marine le pen celebrated his backing as the creation of a great patriotic and republican alliance and said they will campaign hand in hand. it a historic day because we are putting france interests before personal or partisan ones dupont aignan said. macron said their far right and right wing alliance made the campaign battle lines even clearer. there is a reactionary nationalist anti european right wing that has structured itself and which today is an important political force he said. facing it is a progressive bloc that i represent and which defends france. macron is not saying yet who he would name to lead his government if he is elected. in a radio interview saturday he merely said he has people in mind. venturing into rural france to combat le pen arguments that he represents just the big city elite the former economy minister plugged his proposals to reverse the economic and social decline in farming areas. macron promised to modernize phone and internet connections in rural areas and vigorously defended the eu as an essential market for french farmers. on an impromptu tour of the farmers&amp;apos; market in the central town of poitiers macron listened to a grain farmer complain about low price competition from other eu countries and a vegetable farmer lament about the difficulty of getting loans to upgrade farm equipment. as the smell of goat cheeses wafted across the stalls macron rebuffed le pen criticisms of the eu with a vigorous defense of european free trade saying her plans to leave the bloc and its agricultural aid program would spell the end of french farming. rural areas need an open conquering france macron said. our agriculture needs europe and openness. macron promised that no more schools would close in rural areas if he is elected and said his government would intervene directly if mobile operators fail within {DG} months to install high speed fiber optic and phone networks everywhere. le pen has made the plight of french farmers a theme of her campaign citing farm closures rural poverty and farmers&amp;apos; suicides. the tidy village of usseau where macron visited farmer patrick moron on saturday gave {DG} of its votes one third of the total to le pen in round one almost double the {DG} votes it gave to macron. we have wines we have cheeses we had the advantage for a long time said moron a macron supporter. but we are no longer moving forward. neighboring farmer dominique marchand who rotates harvests of colza corn wheat and sunflowers lamented the growing scarcity of rural schools and medical facilities. sometimes we have to go {DG} kilometers {DG} miles to find a doctor or drive {DG} minutes to the nearest emergency room he said. it getting worse and worse. speaking after attending an eu summit on saturday outgoing french president francois hollande warned voters that abstaining on may {DG} could help le pen score high enough to encourage her to run again if she isn&amp;apos;t elected this time.  he said voters on both the left and the right should have no qualms about voting for the centrist macron. it shouldn&amp;apos;t even be a subject of discussion hollande said. a ballot for macron you think of it as the ballot that keeps out the extreme right. abstaining from voting in the runoff hollande added would just encourage le pen in the future. the far right candidate came in third in the presidential race in {DG} and will end up no less than second in {DG}. the lower she is the less strong her ambitions can be for tomorrow hollande said. dupont aignan got {DG}.{DG} percent of the first round vote on april {DG}   compared to macron {DG} percent and {DG} percent for le pen   with a platform that described the eu as inefficient intrusive anti democratic and authoritarian. the right winger called for the eu to be replaced by a community of european states with greater national powers for its members. le pen far right national front rejoiced over the alliance with dupont aignan. florian philippot a national front vice president told bfm television this was a turning point in this campaign. still the alliance caused splits within dupont aignan own party. it prompted the departure of vice president dominique jamet who told bfm that the le pen dupont aignan alliance is a couple that doesn&amp;apos;t please me.     john leicester in paris contributed.  {DG} the associated press.  all rights reserved.  this material may not be published broadcast rewritten or redistributed.  learn more about our privacy policy and terms of use. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working output: dictionary of articles from NYT, Fox, Washington Post\n",
    "for key in ['fox_00', 'nyt_00', 'twp_00', 'apr_00']:\n",
    "    print (key, ':',articles[key],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    if \"u.s.\" in text: text = text.replace(\"u.s.\",\"u<prd>s<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"\" in text: text = text.replace(\".\",\".\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['president trump on tuesday signed an executive order that will make it harder for american tech companies to hire employees from foreign countries willing to work for less money than americans.',\n",
       " 'trump signed the buy american hire american order during his visit to the snap on tools headquarters in kenosha wis.',\n",
       " 'the president called the order bold new steps toward making good on his campaign promises to generate more jobs for out of work americans.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = split_into_sentences(articles['fox_00'])\n",
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scoring News Articles: MNB Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "# Pulls the label with the most number of sentences\n",
    "def maxlab(liberal, conservative, neutral):\n",
    "    if max(liberal, conservative, neutral) == liberal:\n",
    "        return 'liberal'\n",
    "    elif max(liberal, conservative, neutral) == conservative:\n",
    "        return 'conservative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Function to help calculate null hypothesis distribution\n",
    "def myround(x, base=3):\n",
    "    return int(base * round(float(x)/base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intialize vocabulary\n",
    "vect = CountVectorizer()\n",
    "train_vocab = vect.fit_transform(data_train)\n",
    "\n",
    "\n",
    "def mnb_article_score(article_text):\n",
    "    '''\n",
    "    Takes in article text as a string and scores it using the sentence parser and CNN model. \n",
    "    '''\n",
    "    # Get sentences from the article\n",
    "    sentences = split_into_sentences(article_text)\n",
    "    x = len(sentences)\n",
    "    sen = np.asarray(sentences)\n",
    "    \n",
    "    # Building the vocabulary\n",
    "    sen_vocab = vect.transform(sen)\n",
    "\n",
    "    # Predictions based on MNB model\n",
    "    for a in [0.0001]:\n",
    "        mnb = MultinomialNB(alpha=a)\n",
    "        mnb.fit(train_vocab, labs_train)\n",
    "        mnbpreds = mnb.predict(sen_vocab)\n",
    "\n",
    "    # Tally the labels\n",
    "    lib = 0\n",
    "    con = 0\n",
    "    neut = 0\n",
    "    for i in range(mnbpreds.shape[0]):\n",
    "        if mnbpreds[i] == 'Liberal':\n",
    "            lib += 1\n",
    "        elif mnbpreds[i] == 'Conservative':\n",
    "            con += 1\n",
    "        else:\n",
    "            neut += 1\n",
    "\n",
    "    print ('Liberal sentences:', lib)\n",
    "    print ('Conservative sentences:', con)\n",
    "    print ('Neutral sentences:', neut)\n",
    "    print ('Total sentences:', x)\n",
    "\n",
    "\n",
    "    # Assess lean/bias\n",
    "\n",
    "    z = myround(x)\n",
    "    distlabs = [lib,con,neut]\n",
    "    testerlabs = [3,20,10]\n",
    "    normlabs = [z/3,z/3,z/3]\n",
    "\n",
    "    #chisquare to assess normality of label ditribution\n",
    "    pval = sp.stats.chisquare(distlabs,normlabs)[1]\n",
    "    predicted_label = maxlab(lib,con,neut)\n",
    "    print (\"\")\n",
    "    print (\"P-value of chi square:\", pval)\n",
    "    print (\"\")\n",
    "    if pval < .05:\n",
    "        print (\"Article has strong %s lean\" % (predicted_label))\n",
    "    else:\n",
    "        print (\"Article has moderate %s lean\" % (predicted_label))\n",
    "\n",
    "    return predicted_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal sentences: 15\n",
      "Conservative sentences: 20\n",
      "Neutral sentences: 8\n",
      "Total sentences: 43\n",
      "\n",
      "P-value of chi square: 0.0737449428866\n",
      "\n",
      "Article has moderate conservative lean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'conservative'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_article_score(articles['fox_02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through and store article labels\n",
    "mnb_labels = dict()\n",
    "for i in articles.keys():\n",
    "    label = mnb_article_score(articles[i])\n",
    "    mnb_labels[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apr_00': 'liberal',\n",
       " 'apr_01': 'liberal',\n",
       " 'apr_02': 'liberal',\n",
       " 'apr_03': 'conservative',\n",
       " 'apr_04': 'liberal',\n",
       " 'apr_05': 'neutral',\n",
       " 'apr_06': 'liberal',\n",
       " 'apr_07': 'liberal',\n",
       " 'apr_08': 'neutral',\n",
       " 'apr_09': 'liberal',\n",
       " 'apr_10': 'conservative',\n",
       " 'apr_11': 'liberal',\n",
       " 'apr_12': 'liberal',\n",
       " 'apr_13': 'liberal',\n",
       " 'apr_14': 'liberal',\n",
       " 'apr_15': 'liberal',\n",
       " 'apr_16': 'liberal',\n",
       " 'apr_17': 'liberal',\n",
       " 'apr_18': 'liberal',\n",
       " 'apr_19': 'liberal',\n",
       " 'fox_00': 'liberal',\n",
       " 'fox_01': 'neutral',\n",
       " 'fox_02': 'conservative',\n",
       " 'fox_03': 'liberal',\n",
       " 'fox_04': 'neutral',\n",
       " 'fox_05': 'conservative',\n",
       " 'fox_06': 'liberal',\n",
       " 'fox_07': 'neutral',\n",
       " 'fox_08': 'liberal',\n",
       " 'fox_09': 'liberal',\n",
       " 'fox_10': 'liberal',\n",
       " 'fox_11': 'liberal',\n",
       " 'fox_12': 'liberal',\n",
       " 'fox_13': 'liberal',\n",
       " 'fox_14': 'conservative',\n",
       " 'fox_15': 'liberal',\n",
       " 'fox_16': 'liberal',\n",
       " 'fox_17': 'conservative',\n",
       " 'fox_18': 'liberal',\n",
       " 'fox_19': 'liberal',\n",
       " 'nyt_00': 'neutral',\n",
       " 'nyt_01': 'liberal',\n",
       " 'nyt_02': 'liberal',\n",
       " 'nyt_03': 'liberal',\n",
       " 'nyt_04': 'conservative',\n",
       " 'nyt_05': 'conservative',\n",
       " 'nyt_06': 'liberal',\n",
       " 'nyt_07': 'neutral',\n",
       " 'nyt_08': 'neutral',\n",
       " 'nyt_09': 'liberal',\n",
       " 'nyt_10': 'liberal',\n",
       " 'nyt_11': 'liberal',\n",
       " 'nyt_12': 'neutral',\n",
       " 'nyt_13': 'liberal',\n",
       " 'nyt_14': 'conservative',\n",
       " 'nyt_15': 'conservative',\n",
       " 'nyt_16': 'neutral',\n",
       " 'nyt_17': 'liberal',\n",
       " 'nyt_18': 'liberal',\n",
       " 'nyt_19': 'liberal',\n",
       " 'twp_00': 'liberal',\n",
       " 'twp_01': 'conservative',\n",
       " 'twp_02': 'liberal',\n",
       " 'twp_03': 'liberal',\n",
       " 'twp_04': 'liberal',\n",
       " 'twp_05': 'liberal',\n",
       " 'twp_06': 'liberal',\n",
       " 'twp_07': 'liberal',\n",
       " 'twp_08': 'liberal',\n",
       " 'twp_09': 'liberal',\n",
       " 'twp_10': 'liberal',\n",
       " 'twp_11': 'conservative',\n",
       " 'twp_12': 'liberal',\n",
       " 'twp_13': 'liberal',\n",
       " 'twp_14': 'conservative',\n",
       " 'twp_15': 'liberal',\n",
       " 'twp_16': 'liberal',\n",
       " 'twp_17': 'liberal',\n",
       " 'twp_18': 'liberal',\n",
       " 'twp_19': 'liberal'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring News Articles: CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions for the scoring function\n",
    "\n",
    "# Define batching function\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "\n",
    "# Defining a function to evaluate sentences\n",
    "def sentence_eval(sentence):\n",
    "    \"\"\"\n",
    "    Evaluates a sentence with the CNN model and outputs the predicted label. \n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: sentence,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    predictions = sess.run(cnn.predictions, feed_dict) # returns an array of predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_article_score(article_text):\n",
    "    '''\n",
    "    Takes in article text as a string and scores it using the sentence parser and CNN model. \n",
    "    '''\n",
    "    # Get sentences from the article\n",
    "    sentences = split_into_sentences(article_text)\n",
    "    \n",
    "    # Map sentences into CNN vocabulary\n",
    "    max_sentence_len = 86 # max length from training data\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_sentence_len)\n",
    "    cnn_sentences = np.array(list(vocab_processor.fit_transform(sentences)))\n",
    "    \n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(cnn_sentences, batch_size = 1, num_epochs = 1)\n",
    "\n",
    "    # Run through sentences and save labels \n",
    "    labels = []\n",
    "    for batch in batches:\n",
    "        label = sentence_eval(batch)\n",
    "        label = label.tolist()[0]\n",
    "        labels.append(label)\n",
    "\n",
    "    # Tally the labels\n",
    "    # Map: Liberal --> 0, Neutral --> 1, Conservative --> 2\n",
    "\n",
    "    lib = 0\n",
    "    con = 0\n",
    "    neut = 0\n",
    "    for i in labels:\n",
    "        if i == 0:\n",
    "            lib += 1\n",
    "        elif i == 2:\n",
    "            con += 1\n",
    "        else:\n",
    "            neut += 1\n",
    "\n",
    "    total_sentences = lib + neut + con\n",
    "\n",
    "    print ('Liberal sentences:', lib)\n",
    "    print ('Conservative sentences:', con)\n",
    "    print ('Neutral sentences:', neut)\n",
    "    print ('Total sentences:', total_sentences)\n",
    "    \n",
    "    # Define distribution of sentences for CNN model and null hypothesis \n",
    "    z = myround(total_sentences)\n",
    "    distlabs = [lib,con,neut]\n",
    "    normlabs = [z/3,z/3,z/3]\n",
    "\n",
    "    #chisquare to assess normality of label ditribution\n",
    "    pval = sp.stats.chisquare(distlabs,normlabs)[1]\n",
    "    predicted_label = maxlab(lib,con,neut)\n",
    "    print (\"\")\n",
    "    print (\"P-value of chi square:\", pval)\n",
    "    print (\"\")\n",
    "    if pval < .05:\n",
    "        print (\"Article has strong %s lean\" % (predicted_label))\n",
    "    else:\n",
    "        print (\"Article has moderate %s lean\" % (predicted_label))\n",
    "    \n",
    "    return predicted_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal sentences: 7\n",
      "Conservative sentences: 21\n",
      "Neutral sentences: 15\n",
      "Total sentences: 43\n",
      "\n",
      "P-value of chi square: 0.0291379367425\n",
      "\n",
      "Article has strong conservative lean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'conservative'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_article_score(articles['fox_02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through and store article labels\n",
    "cnn_labels = dict()\n",
    "for i in articles.keys():\n",
    "    label = cnn_article_score(articles[i])\n",
    "    cnn_labels[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apr_00': 'liberal',\n",
       " 'apr_01': 'liberal',\n",
       " 'apr_02': 'conservative',\n",
       " 'apr_03': 'neutral',\n",
       " 'apr_04': 'neutral',\n",
       " 'apr_05': 'conservative',\n",
       " 'apr_06': 'conservative',\n",
       " 'apr_07': 'conservative',\n",
       " 'apr_08': 'conservative',\n",
       " 'apr_09': 'neutral',\n",
       " 'apr_10': 'liberal',\n",
       " 'apr_11': 'neutral',\n",
       " 'apr_12': 'conservative',\n",
       " 'apr_13': 'conservative',\n",
       " 'apr_14': 'neutral',\n",
       " 'apr_15': 'conservative',\n",
       " 'apr_16': 'conservative',\n",
       " 'apr_17': 'conservative',\n",
       " 'apr_18': 'conservative',\n",
       " 'apr_19': 'liberal',\n",
       " 'fox_00': 'conservative',\n",
       " 'fox_01': 'neutral',\n",
       " 'fox_02': 'conservative',\n",
       " 'fox_03': 'neutral',\n",
       " 'fox_04': 'conservative',\n",
       " 'fox_05': 'neutral',\n",
       " 'fox_06': 'liberal',\n",
       " 'fox_07': 'conservative',\n",
       " 'fox_08': 'conservative',\n",
       " 'fox_09': 'conservative',\n",
       " 'fox_10': 'conservative',\n",
       " 'fox_11': 'conservative',\n",
       " 'fox_12': 'conservative',\n",
       " 'fox_13': 'neutral',\n",
       " 'fox_14': 'neutral',\n",
       " 'fox_15': 'neutral',\n",
       " 'fox_16': 'neutral',\n",
       " 'fox_17': 'conservative',\n",
       " 'fox_18': 'neutral',\n",
       " 'fox_19': 'neutral',\n",
       " 'nyt_00': 'neutral',\n",
       " 'nyt_01': 'conservative',\n",
       " 'nyt_02': 'conservative',\n",
       " 'nyt_03': 'conservative',\n",
       " 'nyt_04': 'liberal',\n",
       " 'nyt_05': 'conservative',\n",
       " 'nyt_06': 'neutral',\n",
       " 'nyt_07': 'conservative',\n",
       " 'nyt_08': 'neutral',\n",
       " 'nyt_09': 'conservative',\n",
       " 'nyt_10': 'conservative',\n",
       " 'nyt_11': 'neutral',\n",
       " 'nyt_12': 'conservative',\n",
       " 'nyt_13': 'conservative',\n",
       " 'nyt_14': 'conservative',\n",
       " 'nyt_15': 'neutral',\n",
       " 'nyt_16': 'neutral',\n",
       " 'nyt_17': 'neutral',\n",
       " 'nyt_18': 'conservative',\n",
       " 'nyt_19': 'conservative',\n",
       " 'twp_00': 'neutral',\n",
       " 'twp_01': 'liberal',\n",
       " 'twp_02': 'conservative',\n",
       " 'twp_03': 'neutral',\n",
       " 'twp_04': 'conservative',\n",
       " 'twp_05': 'neutral',\n",
       " 'twp_06': 'neutral',\n",
       " 'twp_07': 'conservative',\n",
       " 'twp_08': 'neutral',\n",
       " 'twp_09': 'neutral',\n",
       " 'twp_10': 'conservative',\n",
       " 'twp_11': 'conservative',\n",
       " 'twp_12': 'conservative',\n",
       " 'twp_13': 'conservative',\n",
       " 'twp_14': 'conservative',\n",
       " 'twp_15': 'neutral',\n",
       " 'twp_16': 'conservative',\n",
       " 'twp_17': 'conservative',\n",
       " 'twp_18': 'conservative',\n",
       " 'twp_19': 'neutral'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Predictions of MNB and CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agree: fox_17\n",
      "Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_07\n",
      "MNB Label: neutral\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_06\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_05\n",
      "MNB Label: neutral\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: fox_01\n",
      "Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_04\n",
      "MNB Label: neutral\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_12\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_03\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_09\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_15\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: twp_09\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Agree: twp_14\n",
      "Label: conservative\n",
      "\n",
      "\n",
      "Agree: fox_02\n",
      "Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_09\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_19\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Agree: twp_11\n",
      "Label: conservative\n",
      "\n",
      "\n",
      "Agree: apr_19\n",
      "Label: liberal\n",
      "\n",
      "\n",
      "Disagree: apr_17\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_05\n",
      "MNB Label: conservative\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_16\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_14\n",
      "MNB Label: conservative\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_00\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_00\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: nyt_11\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: nyt_02\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_01\n",
      "MNB Label: conservative\n",
      "CNN Label: liberal\n",
      "\n",
      "\n",
      "Disagree: apr_02\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_03\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_15\n",
      "MNB Label: conservative\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Agree: nyt_14\n",
      "Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_13\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_18\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_02\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_07\n",
      "MNB Label: neutral\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_19\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: apr_04\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: nyt_04\n",
      "MNB Label: conservative\n",
      "CNN Label: liberal\n",
      "\n",
      "\n",
      "Disagree: twp_17\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_04\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_03\n",
      "MNB Label: conservative\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_11\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_07\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: apr_01\n",
      "Label: liberal\n",
      "\n",
      "\n",
      "Disagree: nyt_09\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_12\n",
      "MNB Label: neutral\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_10\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_15\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_08\n",
      "MNB Label: neutral\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: apr_00\n",
      "Label: liberal\n",
      "\n",
      "\n",
      "Disagree: twp_15\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: apr_12\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: nyt_00\n",
      "Label: neutral\n",
      "\n",
      "\n",
      "Agree: fox_06\n",
      "Label: liberal\n",
      "\n",
      "\n",
      "Disagree: twp_08\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: nyt_13\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_10\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_06\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: apr_13\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_18\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: nyt_06\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_13\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: twp_18\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: nyt_17\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: fox_08\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: twp_05\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: apr_16\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_18\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_14\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: twp_07\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_11\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: nyt_01\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: nyt_08\n",
      "Label: neutral\n",
      "\n",
      "\n",
      "Disagree: twp_03\n",
      "MNB Label: liberal\n",
      "CNN Label: neutral\n",
      "\n",
      "\n",
      "Disagree: twp_16\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: nyt_05\n",
      "Label: conservative\n",
      "\n",
      "\n",
      "Disagree: apr_10\n",
      "MNB Label: conservative\n",
      "CNN Label: liberal\n",
      "\n",
      "\n",
      "Disagree: nyt_19\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_10\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Disagree: fox_12\n",
      "MNB Label: liberal\n",
      "CNN Label: conservative\n",
      "\n",
      "\n",
      "Agree: nyt_16\n",
      "Label: neutral\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "disagree = 0\n",
    "for i in articles.keys():\n",
    "    if cnn_labels[i] == mnb_labels[i]:\n",
    "        print(\"Agree:\", i)\n",
    "        print('Label:',cnn_labels[i])\n",
    "        print('\\n')\n",
    "        agree += 1\n",
    "    else:\n",
    "        print(\"Disagree:\", i)\n",
    "        print('MNB Label:',mnb_labels[i])\n",
    "        print('CNN Label:',cnn_labels[i])\n",
    "        print('\\n')\n",
    "        disagree +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN and MNB agree 14 times, and disagree 66 times\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN and MNB agree\", agree, \"times, and disagree\", disagree,\"times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal sentences: 26\n",
      "Conservative sentences: 8\n",
      "Neutral sentences: 12\n",
      "Total sentences: 46\n",
      "\n",
      "P-value of chi square: 0.00256276976307\n",
      "\n",
      "Article has strong liberal lean\n",
      "\n",
      "\n",
      "Liberal sentences: 11\n",
      "Conservative sentences: 15\n",
      "Neutral sentences: 20\n",
      "Total sentences: 46\n",
      "\n",
      "P-value of chi square: 0.254955396027\n",
      "\n",
      "Article has moderate neutral lean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at disagreement samples from different news sources\n",
    "\n",
    "# Fox News\n",
    "mnb_article_score(articles['fox_13'])\n",
    "print('\\n')\n",
    "cnn_article_score(articles['fox_13'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal sentences: 20\n",
      "Conservative sentences: 16\n",
      "Neutral sentences: 16\n",
      "Total sentences: 52\n",
      "\n",
      "P-value of chi square: 0.723590675531\n",
      "\n",
      "Article has moderate liberal lean\n",
      "\n",
      "\n",
      "Liberal sentences: 15\n",
      "Conservative sentences: 19\n",
      "Neutral sentences: 18\n",
      "Total sentences: 52\n",
      "\n",
      "P-value of chi square: 0.767431631972\n",
      "\n",
      "Article has moderate conservative lean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'conservative'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New York Times\n",
    "mnb_article_score(articles['nyt_19'])\n",
    "print('\\n')\n",
    "cnn_article_score(articles['nyt_19'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal sentences: 47\n",
      "Conservative sentences: 17\n",
      "Neutral sentences: 8\n",
      "Total sentences: 72\n",
      "\n",
      "P-value of chi square: 2.84533480898e-08\n",
      "\n",
      "Article has strong liberal lean\n",
      "\n",
      "\n",
      "Liberal sentences: 25\n",
      "Conservative sentences: 30\n",
      "Neutral sentences: 17\n",
      "Total sentences: 72\n",
      "\n",
      "P-value of chi square: 0.166682134478\n",
      "\n",
      "Article has moderate conservative lean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'conservative'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Washington Post\n",
    "mnb_article_score(articles['twp_07'])\n",
    "print('\\n')\n",
    "cnn_article_score(articles['twp_07'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberal sentences: 19\n",
      "Conservative sentences: 9\n",
      "Neutral sentences: 5\n",
      "Total sentences: 33\n",
      "\n",
      "P-value of chi square: 0.00885057608905\n",
      "\n",
      "Article has strong liberal lean\n",
      "\n",
      "\n",
      "Liberal sentences: 8\n",
      "Conservative sentences: 14\n",
      "Neutral sentences: 11\n",
      "Total sentences: 33\n",
      "\n",
      "P-value of chi square: 0.44123316776\n",
      "\n",
      "Article has moderate conservative lean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'conservative'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Associated Press\n",
    "mnb_article_score(articles['apr_18'])\n",
    "print('\\n')\n",
    "cnn_article_score(articles['apr_18'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Distribution of Article Labels for Different News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keys for each source\n",
    "fox_keys = ['fox_00','fox_01','fox_02','fox_03','fox_04','fox_05','fox_06',\n",
    "           'fox_07','fox_08','fox_09','fox_10','fox_11','fox_12', 'fox_13',\n",
    "           'fox_14','fox_15','fox_16','fox_17','fox_18','fox_19']\n",
    "\n",
    "nyt_keys = ['nyt_00','nyt_01','nyt_02','nyt_03','nyt_04','nyt_05','nyt_06',\n",
    "           'nyt_07','nyt_08','nyt_09','nyt_10','nyt_11','nyt_12', 'nyt_13',\n",
    "           'nyt_14','nyt_15','nyt_16','nyt_17','nyt_18','nyt_19']\n",
    "\n",
    "apr_keys = ['apr_00','apr_01','apr_02','apr_03','apr_04','apr_05','apr_06',\n",
    "           'apr_07','apr_08','apr_09','apr_10','apr_11','apr_12', 'apr_13',\n",
    "           'apr_14','apr_15','apr_16','apr_17','apr_18','apr_19']\n",
    "\n",
    "twp_keys = ['twp_00','twp_01','twp_02','twp_03','twp_04','twp_05','twp_06',\n",
    "           'twp_07','twp_08','twp_09','twp_10','twp_11','twp_12', 'twp_13',\n",
    "           'twp_14','twp_15','twp_16','twp_17','twp_18','twp_19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNB labels\n",
    "\n",
    "mnb_fox_labels = []\n",
    "for k in fox_keys:\n",
    "    label = mnb_labels[k]\n",
    "    mnb_fox_labels.append(label)\n",
    "    \n",
    "mnb_nyt_labels = []\n",
    "for k in nyt_keys:\n",
    "    label = mnb_labels[k]\n",
    "    mnb_nyt_labels.append(label)\n",
    "\n",
    "mnb_apr_labels = []\n",
    "for k in apr_keys:\n",
    "    label = mnb_labels[k]\n",
    "    mnb_apr_labels.append(label)\n",
    "    \n",
    "mnb_twp_labels = []\n",
    "for k in twp_keys:\n",
    "    label = mnb_labels[k]\n",
    "    mnb_twp_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CNN labels\n",
    "\n",
    "cnn_fox_labels = []\n",
    "for k in fox_keys:\n",
    "    label = cnn_labels[k]\n",
    "    cnn_fox_labels.append(label)\n",
    "    \n",
    "cnn_nyt_labels = []\n",
    "for k in nyt_keys:\n",
    "    label = cnn_labels[k]\n",
    "    cnn_nyt_labels.append(label)\n",
    "\n",
    "cnn_apr_labels = []\n",
    "for k in apr_keys:\n",
    "    label = cnn_labels[k]\n",
    "    cnn_apr_labels.append(label)\n",
    "    \n",
    "cnn_twp_labels = []\n",
    "for k in twp_keys:\n",
    "    label = cnn_labels[k]\n",
    "    cnn_twp_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_distribution(labels):\n",
    "    '''\n",
    "    Takes a list of labels and returns counts of each category. \n",
    "    '''\n",
    "    c = 0\n",
    "    n = 0\n",
    "    l = 0\n",
    "    for label in labels:\n",
    "        if label == 'conservative':\n",
    "            c +=1\n",
    "        elif label == 'neutral':\n",
    "            n +=1\n",
    "        elif label == 'liberal':\n",
    "            l +=1\n",
    "\n",
    "    print(\"Conservative articles:\",c)\n",
    "    print(\"Neutral articles:\",n)\n",
    "    print(\"Liberal articles:\",l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fox News \n",
      "\n",
      "MNB Classification:\n",
      "Conservative articles: 4\n",
      "Neutral articles: 3\n",
      "Liberal articles: 13\n",
      "\n",
      " CNN Classification:\n",
      "Conservative articles: 10\n",
      "Neutral articles: 9\n",
      "Liberal articles: 1\n"
     ]
    }
   ],
   "source": [
    "print('Fox News','\\n')\n",
    "print(\"MNB Classification:\")\n",
    "article_distribution(mnb_fox_labels)\n",
    "print('\\n',\"CNN Classification:\")\n",
    "article_distribution(cnn_fox_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York Times \n",
      "\n",
      "MNB Classification:\n",
      "Conservative articles: 4\n",
      "Neutral articles: 5\n",
      "Liberal articles: 11\n",
      "\n",
      " CNN Classification:\n",
      "Conservative articles: 12\n",
      "Neutral articles: 7\n",
      "Liberal articles: 1\n"
     ]
    }
   ],
   "source": [
    "print('New York Times','\\n')\n",
    "print(\"MNB Classification:\")\n",
    "article_distribution(mnb_nyt_labels)\n",
    "print('\\n',\"CNN Classification:\")\n",
    "article_distribution(cnn_nyt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associated Press \n",
      "\n",
      "MNB Classification:\n",
      "Conservative articles: 2\n",
      "Neutral articles: 2\n",
      "Liberal articles: 16\n",
      "\n",
      " CNN Classification:\n",
      "Conservative articles: 11\n",
      "Neutral articles: 5\n",
      "Liberal articles: 4\n"
     ]
    }
   ],
   "source": [
    "print('Associated Press','\\n')\n",
    "print(\"MNB Classification:\")\n",
    "article_distribution(mnb_apr_labels)\n",
    "print('\\n',\"CNN Classification:\")\n",
    "article_distribution(cnn_apr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Washington Post \n",
      "\n",
      "MNB Classification:\n",
      "Conservative articles: 3\n",
      "Neutral articles: 0\n",
      "Liberal articles: 17\n",
      "\n",
      " CNN Classification:\n",
      "Conservative articles: 11\n",
      "Neutral articles: 8\n",
      "Liberal articles: 1\n"
     ]
    }
   ],
   "source": [
    "print('The Washington Post','\\n')\n",
    "print(\"MNB Classification:\")\n",
    "article_distribution(mnb_twp_labels)\n",
    "print('\\n',\"CNN Classification:\")\n",
    "article_distribution(cnn_twp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
